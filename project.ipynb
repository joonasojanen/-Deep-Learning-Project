{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 521153S Deep Learning Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152, ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights, vgg11, vgg13, vgg16, vgg19, VGG11_Weights, VGG13_Weights, VGG16_Weights, VGG19_Weights\n",
    "import gdown\n",
    "import urllib\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import time\n",
    "import shutil\n",
    "import logging as lg\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to run this if there is nood for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logging level\n",
    "# Possible Levels: DEBUG, INFO,\n",
    "lg.basicConfig(level=lg.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the device to use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# If you encounter some issues regarding cuda device, e.g., \"RuntimeError: CUDA Out of memory error\",\n",
    "# try to switch the device to cpu by using the following code\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Size\n",
    "image_size = 224\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers (id not good, decrease learning rate (try 0.001, 0.0001, 0.00001))\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Weight decay (if not good, increase weight_decay (try 1e-4, 1e-3, 1e-2))\n",
    "weight_decay=1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the necessary the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "val_url = 'https://drive.google.com/u/0/uc?id=1hSMUMj5IRpf-nQs1OwgiQLmGZCN0KDWl'\n",
    "train_url = 'https://drive.google.com/u/0/uc?id=107FTosYIeBn5QbynR46YG91nHcJ70whs'\n",
    "test_url = 'https://drive.google.com/u/0/uc?id=1yKyKgxcnGMIAnA_6Vr2ilbpHMc9COg-v'\n",
    "eurosat_url = 'https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1'\n",
    "cub_url = 'https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1'\n",
    "\n",
    "val_file = './data/val.tar'\n",
    "train_file = './data/train.tar'\n",
    "test_file = './data/test.tar'\n",
    "eurosat_file = './data/eurosatrgb.zip'\n",
    "cub_file = './data/cub_200_2011.tgz'\n",
    "\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    print('Creating data directory')\n",
    "    os.mkdir('./data')\n",
    "\n",
    "if not os.path.exists('./data/val.tar'):\n",
    "    print('Downloading val.tar')\n",
    "    gdown.download(val_url, val_file)\n",
    "    val_tar = tarfile.open(val_file)\n",
    "    val_tar.extractall('./data/')\n",
    "    val_tar.close()\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print('Downloading train.tar')\n",
    "    gdown.download(train_url, train_file)\n",
    "    train_tar = tarfile.open(train_file)\n",
    "    train_tar.extractall('./data/')\n",
    "    train_tar.close()\n",
    "\n",
    "if not os.path.exists(test_file):\n",
    "    print('Downloading test.tar')\n",
    "    gdown.download(test_url, test_file)\n",
    "    test_tar = tarfile.open(test_file)\n",
    "    test_tar.extractall('./data/')\n",
    "    test_tar.close()\n",
    "\n",
    "if not os.path.exists(eurosat_file):\n",
    "    print('Downloading EuroSAT_RGB.zip')\n",
    "    response = urllib.request.urlretrieve(eurosat_url, eurosat_file)\n",
    "    eurosat_zip = zipfile.ZipFile(eurosat_file)\n",
    "    eurosat_zip.extractall('./data/eurosat')\n",
    "    eurosat_zip.close()\n",
    "\n",
    "if not os.path.exists(cub_file):\n",
    "    print('Downloading cub200_2011.tgz')\n",
    "    response = urllib.request.urlretrieve(cub_url, cub_file)\n",
    "    tar = tarfile.open(cub_file, 'r')\n",
    "    for item in tar:\n",
    "        tar.extract(item, './data')\n",
    "    tar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model_resnet34 = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "model_resnet50 = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "model_vgg11 = vgg11(weights=VGG11_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 26880\n",
      "Number of samples in the validation set: 5760\n",
      "Number of samples in the test set: 5760\n"
     ]
    }
   ],
   "source": [
    "# Stratified split, tries to maintain a proportional distribution of samples for each class in all three sets (train, val and test).\n",
    "\n",
    "# Val and Test ratio total, Train ratio (1-test_ratio)\n",
    "test_ratio = 0.3\n",
    "\n",
    "# Split between Val and Test\n",
    "test_val_split = 0.5\n",
    "\n",
    "# Loading the ImageFolder dataset\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='./data/train')\n",
    "\n",
    "# Getting the number of classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Initializing lists to store indices for train, validation, and test sets\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "\n",
    "# Splitting each class separately\n",
    "for class_index in range(num_classes):\n",
    "    # Getting indices for samples belonging to the current class\n",
    "    class_indices = np.where(np.array(train_dataset.targets) == class_index)[0]\n",
    "    \n",
    "    # Splitting indices for each class separately into train, validation, and test sets\n",
    "    train_idx, temp_idx = train_test_split(class_indices, test_size=test_ratio, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=test_val_split, random_state=42)\n",
    "    \n",
    "    # Extending the lists to store indices for each set\n",
    "    train_indices.extend(train_idx)\n",
    "    val_indices.extend(val_idx)\n",
    "    test_indices.extend(test_idx)\n",
    "\n",
    "# Creating Subset objects using the selected indices for each set\n",
    "train_set = Subset(train_dataset, train_indices)\n",
    "val_set = Subset(train_dataset, val_indices)\n",
    "test_set = Subset(train_dataset, test_indices)\n",
    "\n",
    "# Transform functions\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(image_size),      \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "])\n",
    "\n",
    "train_set.dataset.transform = transform_train\n",
    "val_set.dataset.transform = transform_eval\n",
    "test_set.dataset.transform = transform_eval\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Printing the sizes of the resulting sets\n",
    "print(f\"Number of samples in the training set: {len(train_set)}\")\n",
    "print(f\"Number of samples in the validation set: {len(val_set)}\")\n",
    "print(f\"Number of samples in the test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the used model\n",
    "model = model_resnet18\n",
    "model.name = \"resnet18\"     # Set name for saving\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, criterion):\n",
    "\n",
    "    model.eval() \n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    num_images = 0.0 # used to accumulate number of images\n",
    "    \n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        counter += 1\n",
    "        \n",
    "        image, labels = data\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass.\n",
    "        outputs = model(image)\n",
    "        # Calculate the loss.\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Calculate the accuracy.\n",
    "        preds = torch.max(outputs, 1).indices\n",
    "        valid_running_correct += torch.sum(preds == labels.data)\n",
    "        num_images += len(labels)\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    lg.debug(f\"EVAL:   loss.item(): {loss.item()} valid_running_correct: {valid_running_correct.item()}\")\n",
    "\n",
    "    epoch_acc = valid_running_correct / num_images\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainLoader, valLoader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    # Start the training.\n",
    "    if (torch.cuda.is_available() and device.type == 'cuda'):\n",
    "        print(\"Using CUDA\")\n",
    "\n",
    "    print('Training')\n",
    "    \n",
    "    lg.debug(f\" len(trainLoader.dataset): {len(trainLoader.dataset)}\")\n",
    "    lg.debug(f\" len(valLoader.dataset): {len(valLoader.dataset)}\")\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_running_loss = 0.0\n",
    "        train_running_correct = 0\n",
    "        counter = 0\n",
    "        num_images = 0.0 # used to accumulate number of images\n",
    "        for i, data in enumerate(trainLoader,0):\n",
    "            counter += 1\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            train_running_correct += torch.sum(preds == labels.data)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            # Update the weights.\n",
    "            optimizer.step()    # Output training \n",
    "            num_images += len(image)\n",
    "            if counter % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}/{num_epochs}, Step: {counter}/{len(trainLoader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        valid_epoch_acc = evaluate(model, valLoader, device, criterion)\n",
    "        lg.debug(f\"TRAIN: loss.item(): {loss.item()}, train_running_correct: {train_running_correct}\")\n",
    "        \n",
    "        train_epoch_acc = train_running_correct  / num_images\n",
    "        print('Epoch: %d/%d, Training accuracy: %f, Loss: %f, Validation accuracy: %f' % (epoch+1, num_epochs, train_epoch_acc, loss.item(), valid_epoch_acc))\n",
    "        print('-'*50)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/10, Step: 100/210, Loss: 2.8664\n",
      "Epoch: 1/10, Step: 200/210, Loss: 1.6076\n",
      "Epoch: 1/10, Training accuracy: 0.417820, Loss: 1.677926, Validation accuracy: 0.699479\n",
      "--------------------------------------------------\n",
      "Epoch: 2/10, Step: 100/210, Loss: 1.2053\n",
      "Epoch: 2/10, Step: 200/210, Loss: 0.9176\n",
      "Epoch: 2/10, Training accuracy: 0.759747, Loss: 1.067810, Validation accuracy: 0.783681\n",
      "--------------------------------------------------\n",
      "Epoch: 3/10, Step: 100/210, Loss: 0.8008\n",
      "Epoch: 3/10, Step: 200/210, Loss: 0.7590\n",
      "Epoch: 3/10, Training accuracy: 0.818638, Loss: 0.829025, Validation accuracy: 0.813194\n",
      "--------------------------------------------------\n",
      "Epoch: 4/10, Step: 100/210, Loss: 0.5513\n",
      "Epoch: 4/10, Step: 200/210, Loss: 0.6269\n",
      "Epoch: 4/10, Training accuracy: 0.853981, Loss: 0.520096, Validation accuracy: 0.831771\n",
      "--------------------------------------------------\n",
      "Epoch: 5/10, Step: 100/210, Loss: 0.4291\n",
      "Epoch: 5/10, Step: 200/210, Loss: 0.6209\n",
      "Epoch: 5/10, Training accuracy: 0.878944, Loss: 0.568682, Validation accuracy: 0.837847\n",
      "--------------------------------------------------\n",
      "Epoch: 6/10, Step: 100/210, Loss: 0.5114\n",
      "Epoch: 6/10, Step: 200/210, Loss: 0.5106\n",
      "Epoch: 6/10, Training accuracy: 0.900186, Loss: 0.440615, Validation accuracy: 0.841319\n",
      "--------------------------------------------------\n",
      "Epoch: 7/10, Step: 100/210, Loss: 0.3459\n",
      "Epoch: 7/10, Step: 200/210, Loss: 0.4285\n",
      "Epoch: 7/10, Training accuracy: 0.917746, Loss: 0.370340, Validation accuracy: 0.847396\n",
      "--------------------------------------------------\n",
      "Epoch: 8/10, Step: 100/210, Loss: 0.2965\n",
      "Epoch: 8/10, Step: 200/210, Loss: 0.2881\n",
      "Epoch: 8/10, Training accuracy: 0.933110, Loss: 0.291811, Validation accuracy: 0.850694\n",
      "--------------------------------------------------\n",
      "Epoch: 9/10, Step: 100/210, Loss: 0.2459\n",
      "Epoch: 9/10, Step: 200/210, Loss: 0.3017\n",
      "Epoch: 9/10, Training accuracy: 0.947284, Loss: 0.239530, Validation accuracy: 0.852257\n",
      "--------------------------------------------------\n",
      "Epoch: 10/10, Step: 100/210, Loss: 0.2082\n",
      "Epoch: 10/10, Step: 200/210, Loss: 0.2056\n",
      "Epoch: 10/10, Training accuracy: 0.959338, Loss: 0.224145, Validation accuracy: 0.855729\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(trained_model):\n",
    "    # Create Models Folder\n",
    "    if not os.path.exists('./models'):\n",
    "        print('Creating models directory')\n",
    "        os.mkdir('./models')\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists(f'./models/{trained_model.name}.pth'):\n",
    "        print(f'Saving Model {trained_model.name}')\n",
    "        torch.save(trained_model, f'./models/{trained_model.name}.pth')\n",
    "\n",
    "def load_model(model_name):\n",
    "    if not os.path.exists(f'./models/{model_name}.pth'):\n",
    "        print(f'Cannot Load {model_name}')\n",
    "        return None\n",
    "    print(f'Loading Model {model_name}')\n",
    "    model = torch.load(f'./models/{model_name}.pth')\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pre-trained model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model resnet18\n"
     ]
    }
   ],
   "source": [
    "save_model(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalue the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.846\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate(pretrained_model, test_loader, device, criterion)\n",
    "print(f\"test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EuroSAT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the 100 images from EuroSAT dataset for the training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_100_eurosat():\n",
    "    '''\n",
    "    The `choose_100_eurosat()` function selects 100 images from the EuroSAT dataset and copies them to the `data/eurosat_validation` folder. It also creates a `data/eurosat_training` folder if it doesn't already exist.\n",
    "    '''\n",
    "    if not os.path.exists('./data/eurosat_validation'):\n",
    "        print('Creating data directory')\n",
    "        os.mkdir('./data/eurosat_validation')\n",
    "    else:\n",
    "        # Folder full of images -> delete all files in the directory\n",
    "        shutil.rmtree(os.path.join('./data/eurosat_validation'))\n",
    "        os.mkdir('./data/eurosat_validation')\n",
    "\n",
    "    if not os.path.exists('./data/eurosat_training'):\n",
    "        print('Creating training directory')\n",
    "        os.mkdir('./data/eurosat_training')\n",
    "    else:\n",
    "        # Folder full of images -> delete all files in the directory\n",
    "        shutil.rmtree(os.path.join('./data/eurosat_training'))\n",
    "        os.mkdir('./data/eurosat_training')\n",
    "\n",
    "    # Load the EuroSAT Categories\n",
    "    eurosat_categories = [name for name in os.listdir('./data/eurosat/EuroSAT_RGB/') if os.path.isdir(os.path.join('./data/eurosat/EuroSAT_RGB/', name))]\n",
    "    # Randomly select 5 categories\n",
    "    lg.debug(f\"Selecting 5 categories from {eurosat_categories} categories\")\n",
    "    selected_categories = np.random.choice(eurosat_categories, 5, replace=False)\n",
    "    lg.debug(f\"Selected categories: {selected_categories}\")\n",
    "\n",
    "    selected_images = []\n",
    "    training_images = []\n",
    "\n",
    "    # From each directory, randomly select 20 images\n",
    "    for category in selected_categories:\n",
    "        images = os.listdir(os.path.join('./data/eurosat/EuroSAT_RGB/', category))\n",
    "        selected = random.sample(images, 20)\n",
    "        selected_images.extend([(category, image) for image in selected])\n",
    "        lg.debug(f\"Selected {selected} from {category}\")\n",
    "        # Copy selected images to the selected directory\n",
    "        for image in selected:\n",
    "            if not os.path.exists(f\"./data/eurosat_validation/{category}\"):\n",
    "                lg.debug(f\"Creating {category} directory\")\n",
    "                os.mkdir(f\"./data/eurosat_validation/{category}\")\n",
    "            shutil.copyfile(os.path.join('./data/eurosat/EuroSAT_RGB', category, image), os.path.join(f\"./data/eurosat_validation/{category}\", image))\n",
    "\n",
    "    # From these 100 images, randomly select 5 images from each category for the training set\n",
    "    for category in selected_categories:\n",
    "        category_images = [image for (cat, image) in selected_images if cat == category]\n",
    "        training = random.sample(category_images, 5)\n",
    "        training_images.extend(training)\n",
    "        lg.debug(f\"Selected {training} for training\")\n",
    "\n",
    "        # Copy training images to the training directory\n",
    "        for image in training:\n",
    "            if not os.path.exists(f\"./data/eurosat_training/{category}\"):\n",
    "                lg.debug(f\"Creating {category} directory\")\n",
    "                os.mkdir(f\"./data/eurosat_training/{category}\")\n",
    "            shutil.move(os.path.join(f\"./data/eurosat_validation/{category}\", image), os.path.join(f\"./data/eurosat_training/{category}\", image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_euro_datasets():\n",
    "\n",
    "    # Choose 100 images from EuroSAT dataset\n",
    "    choose_100_eurosat()\n",
    "\n",
    "    eurosat_train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root='./data/eurosat_training',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    eurosat_validation_dataset = torchvision.datasets.ImageFolder(\n",
    "        root='./data/eurosat_validation',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    eurosat_dataloader_train = DataLoader(eurosat_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    eurosat_dataloader_validation = DataLoader(eurosat_validation_dataset, batch_size=batch_size, shuffle = False, num_workers=num_workers)\n",
    "\n",
    "    return eurosat_dataloader_train, eurosat_dataloader_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on EuroSAT training dataset and validate on the EuroSAT validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model resnet18\n",
      "File 'results.txt' found and cleared.\n",
      "Training EuroSAT model 1/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.360000, Loss: 1.455138, Validation accuracy: 0.253333\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.320000, Loss: 0.856582, Validation accuracy: 0.213333\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.200000, Loss: 2.561901, Validation accuracy: 0.226667\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.240000, Loss: 1.166750, Validation accuracy: 0.213333\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.240000, Loss: 0.437903, Validation accuracy: 0.200000\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.200000, Loss: 2.344654, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.440000, Loss: 1.855628, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.440000, Loss: 3.353392, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.520000, Loss: 0.772419, Validation accuracy: 0.373333\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.600000, Loss: 1.952183, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.680000, Loss: 1.689572, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.680000, Loss: 3.143189, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.720000, Loss: 1.575578, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.840000, Loss: 1.243239, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.640000, Loss: 0.543194, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.520000, Loss: 2.231596, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.680000, Loss: 2.209697, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.840000, Loss: 0.967851, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.640000, Loss: 2.360571, Validation accuracy: 0.413333\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.640000, Loss: 4.550949, Validation accuracy: 0.693333\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.760000, Loss: 3.072621, Validation accuracy: 0.680000\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.720000, Loss: 2.773597, Validation accuracy: 0.640000\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.840000, Loss: 2.872302, Validation accuracy: 0.640000\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.760000, Loss: 2.062971, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.800000, Loss: 2.129554, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.760000, Loss: 1.390425, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.840000, Loss: 2.428210, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.720000, Loss: 2.262334, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.800000, Loss: 2.182822, Validation accuracy: 0.706667\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.840000, Loss: 1.099730, Validation accuracy: 0.693333\n",
      "--------------------------------------------------\n",
      "EuroSAT model 1/5 results: 0.693\n",
      "Training EuroSAT model 2/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.080000, Loss: 1.888585, Validation accuracy: 0.146667\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.080000, Loss: 1.195602, Validation accuracy: 0.080000\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.080000, Loss: 2.088271, Validation accuracy: 0.013333\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.040000, Loss: 1.933006, Validation accuracy: 0.040000\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.080000, Loss: 2.605918, Validation accuracy: 0.146667\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.080000, Loss: 1.463330, Validation accuracy: 0.173333\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.280000, Loss: 0.998205, Validation accuracy: 0.213333\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.240000, Loss: 2.625053, Validation accuracy: 0.333333\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.280000, Loss: 1.267878, Validation accuracy: 0.386667\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.440000, Loss: 4.367010, Validation accuracy: 0.266667\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.360000, Loss: 2.195732, Validation accuracy: 0.333333\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.520000, Loss: 0.345382, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.360000, Loss: 3.338836, Validation accuracy: 0.293333\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.400000, Loss: 1.034184, Validation accuracy: 0.333333\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.480000, Loss: 3.269600, Validation accuracy: 0.426667\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.560000, Loss: 0.632319, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.520000, Loss: 5.562901, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.760000, Loss: 0.680917, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.720000, Loss: 2.457835, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.800000, Loss: 2.830286, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.760000, Loss: 2.105272, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.840000, Loss: 1.229280, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.800000, Loss: 2.529265, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.680000, Loss: 2.046882, Validation accuracy: 0.680000\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.800000, Loss: 1.525417, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.840000, Loss: 1.568534, Validation accuracy: 0.640000\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.960000, Loss: 2.724946, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.840000, Loss: 2.912657, Validation accuracy: 0.720000\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.920000, Loss: 2.928862, Validation accuracy: 0.640000\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.760000, Loss: 2.192344, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "EuroSAT model 2/5 results: 0.653\n",
      "Training EuroSAT model 3/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.440000, Loss: 2.914602, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.480000, Loss: 0.610619, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.360000, Loss: 1.614898, Validation accuracy: 0.466667\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.360000, Loss: 0.454156, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.400000, Loss: 0.149689, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.440000, Loss: 0.217311, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.360000, Loss: 2.306077, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.400000, Loss: 1.164552, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.400000, Loss: 2.155267, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.520000, Loss: 1.491921, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.520000, Loss: 3.479655, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.640000, Loss: 1.803670, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.600000, Loss: 2.376791, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.520000, Loss: 0.480845, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.600000, Loss: 1.946687, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.600000, Loss: 1.099136, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.560000, Loss: 2.792212, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.600000, Loss: 0.862179, Validation accuracy: 0.706667\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.680000, Loss: 3.048285, Validation accuracy: 0.773333\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.720000, Loss: 2.997620, Validation accuracy: 0.746667\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.800000, Loss: 1.512302, Validation accuracy: 0.680000\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.760000, Loss: 0.873210, Validation accuracy: 0.720000\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.680000, Loss: 2.243533, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.760000, Loss: 3.716173, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.760000, Loss: 3.495073, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.800000, Loss: 2.341517, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.680000, Loss: 1.898787, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.840000, Loss: 2.423059, Validation accuracy: 0.746667\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.760000, Loss: 1.274717, Validation accuracy: 0.773333\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.640000, Loss: 4.732545, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "EuroSAT model 3/5 results: 0.653\n",
      "Training EuroSAT model 4/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.320000, Loss: 0.376914, Validation accuracy: 0.386667\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.360000, Loss: 2.238953, Validation accuracy: 0.373333\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.440000, Loss: 1.103315, Validation accuracy: 0.253333\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.240000, Loss: 3.956041, Validation accuracy: 0.226667\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.280000, Loss: 3.217244, Validation accuracy: 0.306667\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.360000, Loss: 1.456863, Validation accuracy: 0.373333\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.400000, Loss: 2.703070, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.320000, Loss: 3.229952, Validation accuracy: 0.413333\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.280000, Loss: 2.292445, Validation accuracy: 0.320000\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.480000, Loss: 0.763942, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.320000, Loss: 3.371803, Validation accuracy: 0.413333\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.400000, Loss: 0.491044, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.400000, Loss: 1.796922, Validation accuracy: 0.426667\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.560000, Loss: 2.208645, Validation accuracy: 0.453333\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.480000, Loss: 1.914113, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.520000, Loss: 1.927629, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.720000, Loss: 0.893873, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.560000, Loss: 3.266590, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.640000, Loss: 0.977094, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.760000, Loss: 1.963244, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.560000, Loss: 1.094386, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.800000, Loss: 3.659758, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.840000, Loss: 1.005168, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.720000, Loss: 2.078192, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.480000, Loss: 1.211114, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.800000, Loss: 0.819496, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.720000, Loss: 3.743636, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.720000, Loss: 2.388978, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.760000, Loss: 2.077898, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.680000, Loss: 3.468632, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "EuroSAT model 4/5 results: 0.560\n",
      "Training EuroSAT model 5/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.320000, Loss: 0.869779, Validation accuracy: 0.266667\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.240000, Loss: 0.470134, Validation accuracy: 0.293333\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.400000, Loss: 2.285498, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.480000, Loss: 1.992657, Validation accuracy: 0.386667\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.440000, Loss: 2.609120, Validation accuracy: 0.413333\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.480000, Loss: 4.603908, Validation accuracy: 0.373333\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.360000, Loss: 2.715842, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.480000, Loss: 0.707356, Validation accuracy: 0.386667\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.480000, Loss: 0.166279, Validation accuracy: 0.373333\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.480000, Loss: 3.887316, Validation accuracy: 0.386667\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.520000, Loss: 3.846880, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.720000, Loss: 1.217470, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.720000, Loss: 2.673569, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.680000, Loss: 1.164005, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.640000, Loss: 2.241056, Validation accuracy: 0.346667\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.560000, Loss: 2.884108, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.560000, Loss: 0.754845, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.760000, Loss: 1.043006, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.760000, Loss: 4.535193, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.720000, Loss: 1.849179, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.720000, Loss: 4.975629, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.880000, Loss: 2.813898, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.800000, Loss: 1.876241, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.760000, Loss: 2.574680, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.760000, Loss: 2.267234, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.880000, Loss: 2.106109, Validation accuracy: 0.733333\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.880000, Loss: 4.653107, Validation accuracy: 0.706667\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.800000, Loss: 0.665720, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.680000, Loss: 3.013658, Validation accuracy: 0.640000\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.760000, Loss: 2.237232, Validation accuracy: 0.640000\n",
      "--------------------------------------------------\n",
      "EuroSAT model 5/5 results: 0.640\n",
      "Average results: 0.640\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model_resnet18_fine = load_model(\"resnet18\")\n",
    "\n",
    "for param in model_resnet18_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet18_fine.fc = nn.Linear(model_resnet18_fine.fc.in_features, 5)\n",
    "\n",
    "# Define loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_resnet18_fine.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Load the EuroSAT dataset\n",
    "eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "\n",
    "if os.path.exists(\"results.txt\"):\n",
    "    # If the file exists, clear its contents\n",
    "    with open(\"results.txt\", \"w\") as f:\n",
    "        f.truncate(0)  # Clear the file content\n",
    "        print(\"File 'results.txt' found and cleared.\")\n",
    "\n",
    "# Train and Evaluate the model on the EuroSAT dataset\n",
    "i = 5\n",
    "average_results = 0.0\n",
    "\n",
    "for j in range(i):\n",
    "    print(f\"Training EuroSAT model {j+1}/{i}\")\n",
    "    eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "    model_resnet18_fine = train(model_resnet18_fine, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n",
    "    results = evaluate(model_resnet18_fine, eurosat_dataloader_validation, device, criterion)\n",
    "    average_results += results\n",
    "    print(f\"EuroSAT model {j+1}/{i} results: {results:.3f}\")\n",
    "    # Save the results to results.txt\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{model_resnet18_fine.name} {j} Time: {time.strftime('%d/%m/%Y, %H:%M:%S')} Epochs: {num_epochs} | Results: {results:.6f}\\n\")\n",
    "\n",
    "average_results /= i\n",
    "print(f\"Average results: {average_results:.3f}\")\n",
    "with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{model_resnet18_fine.name} Average results: {average_results:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/10, Step: 100/210, Loss: 2.4471\n",
      "Epoch: 1/10, Step: 200/210, Loss: 1.2354\n",
      "Epoch: 1/10, Training accuracy: 0.502418, Loss: 1.311226, Validation accuracy: 0.778819\n",
      "--------------------------------------------------\n",
      "Epoch: 2/10, Step: 100/210, Loss: 0.7102\n",
      "Epoch: 2/10, Step: 200/210, Loss: 0.5331\n",
      "Epoch: 2/10, Training accuracy: 0.827493, Loss: 0.716467, Validation accuracy: 0.839931\n",
      "--------------------------------------------------\n",
      "Epoch: 3/10, Step: 100/210, Loss: 0.4549\n",
      "Epoch: 3/10, Step: 200/210, Loss: 0.4533\n",
      "Epoch: 3/10, Training accuracy: 0.877679, Loss: 0.456138, Validation accuracy: 0.857986\n",
      "--------------------------------------------------\n",
      "Epoch: 4/10, Step: 100/210, Loss: 0.3867\n",
      "Epoch: 4/10, Step: 200/210, Loss: 0.2645\n",
      "Epoch: 4/10, Training accuracy: 0.910975, Loss: 0.387957, Validation accuracy: 0.869965\n",
      "--------------------------------------------------\n",
      "Epoch: 5/10, Step: 100/210, Loss: 0.3474\n",
      "Epoch: 5/10, Step: 200/210, Loss: 0.3069\n",
      "Epoch: 5/10, Training accuracy: 0.936756, Loss: 0.299334, Validation accuracy: 0.873090\n",
      "--------------------------------------------------\n",
      "Epoch: 6/10, Step: 100/210, Loss: 0.2106\n",
      "Epoch: 6/10, Step: 200/210, Loss: 0.2847\n",
      "Epoch: 6/10, Training accuracy: 0.954985, Loss: 0.237577, Validation accuracy: 0.874653\n",
      "--------------------------------------------------\n",
      "Epoch: 7/10, Step: 100/210, Loss: 0.1331\n",
      "Epoch: 7/10, Step: 200/210, Loss: 0.2246\n",
      "Epoch: 7/10, Training accuracy: 0.969755, Loss: 0.154661, Validation accuracy: 0.879340\n",
      "--------------------------------------------------\n",
      "Epoch: 8/10, Step: 100/210, Loss: 0.0734\n",
      "Epoch: 8/10, Step: 200/210, Loss: 0.1732\n",
      "Epoch: 8/10, Training accuracy: 0.980097, Loss: 0.110809, Validation accuracy: 0.877257\n",
      "--------------------------------------------------\n",
      "Epoch: 9/10, Step: 100/210, Loss: 0.0897\n",
      "Epoch: 9/10, Step: 200/210, Loss: 0.1043\n",
      "Epoch: 9/10, Training accuracy: 0.986607, Loss: 0.101881, Validation accuracy: 0.877083\n",
      "--------------------------------------------------\n",
      "Epoch: 10/10, Step: 100/210, Loss: 0.1067\n",
      "Epoch: 10/10, Step: 200/210, Loss: 0.0731\n",
      "Epoch: 10/10, Training accuracy: 0.992002, Loss: 0.070614, Validation accuracy: 0.877083\n",
      "--------------------------------------------------\n",
      "test acc: 0.874\n",
      "Loading Model resnet34\n",
      "Training EuroSAT model 1/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.080000, Loss: 1.880011, Validation accuracy: 0.293333\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.360000, Loss: 1.800259, Validation accuracy: 0.333333\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.240000, Loss: 2.125731, Validation accuracy: 0.306667\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.320000, Loss: 1.838620, Validation accuracy: 0.213333\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.520000, Loss: 2.092220, Validation accuracy: 0.346667\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.520000, Loss: 1.595086, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.360000, Loss: 0.972072, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.520000, Loss: 2.854393, Validation accuracy: 0.453333\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.640000, Loss: 1.399992, Validation accuracy: 0.453333\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.520000, Loss: 2.283177, Validation accuracy: 0.346667\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.440000, Loss: 0.848736, Validation accuracy: 0.453333\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.560000, Loss: 3.347074, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.680000, Loss: 1.317896, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.600000, Loss: 2.330354, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.760000, Loss: 2.103935, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.760000, Loss: 2.145885, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.640000, Loss: 0.838704, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.720000, Loss: 2.603591, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.760000, Loss: 1.642833, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.840000, Loss: 2.331006, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.840000, Loss: 1.123163, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.880000, Loss: 2.424126, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.880000, Loss: 1.108180, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.760000, Loss: 3.237870, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.720000, Loss: 1.154199, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.720000, Loss: 1.794077, Validation accuracy: 0.453333\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.760000, Loss: 1.801852, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.920000, Loss: 0.685066, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.680000, Loss: 3.582775, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.680000, Loss: 1.921070, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "EuroSAT model 1/5 results: 0.587\n",
      "Training EuroSAT model 2/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.360000, Loss: 2.350730, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.280000, Loss: 3.262999, Validation accuracy: 0.426667\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.280000, Loss: 1.642482, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.400000, Loss: 0.458240, Validation accuracy: 0.266667\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.280000, Loss: 0.222317, Validation accuracy: 0.266667\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.280000, Loss: 2.398601, Validation accuracy: 0.333333\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.440000, Loss: 3.278391, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.440000, Loss: 1.356467, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.440000, Loss: 2.707670, Validation accuracy: 0.333333\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.400000, Loss: 3.287967, Validation accuracy: 0.413333\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.480000, Loss: 1.830195, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.440000, Loss: 1.560537, Validation accuracy: 0.373333\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.400000, Loss: 4.291915, Validation accuracy: 0.440000\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.440000, Loss: 1.420442, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.640000, Loss: 0.888257, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.640000, Loss: 3.205648, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.520000, Loss: 2.941588, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.600000, Loss: 2.280310, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.560000, Loss: 1.171184, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.800000, Loss: 0.858329, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.680000, Loss: 1.358800, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.560000, Loss: 3.797478, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.840000, Loss: 3.290025, Validation accuracy: 0.680000\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.600000, Loss: 2.004384, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.760000, Loss: 3.081011, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.640000, Loss: 0.911609, Validation accuracy: 0.640000\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.760000, Loss: 3.221276, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.600000, Loss: 1.743897, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.840000, Loss: 0.291198, Validation accuracy: 0.453333\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.680000, Loss: 4.381008, Validation accuracy: 0.386667\n",
      "--------------------------------------------------\n",
      "EuroSAT model 2/5 results: 0.387\n",
      "Training EuroSAT model 3/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.200000, Loss: 2.554125, Validation accuracy: 0.053333\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.040000, Loss: 1.824665, Validation accuracy: 0.133333\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.080000, Loss: 2.888975, Validation accuracy: 0.093333\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.120000, Loss: 5.019134, Validation accuracy: 0.093333\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.120000, Loss: 1.271075, Validation accuracy: 0.066667\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.160000, Loss: 1.121951, Validation accuracy: 0.053333\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.240000, Loss: 3.216636, Validation accuracy: 0.200000\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.320000, Loss: 0.780294, Validation accuracy: 0.280000\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.320000, Loss: 1.257805, Validation accuracy: 0.333333\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.360000, Loss: 3.511172, Validation accuracy: 0.253333\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.320000, Loss: 2.359449, Validation accuracy: 0.293333\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.440000, Loss: 2.279167, Validation accuracy: 0.333333\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.400000, Loss: 1.223908, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.560000, Loss: 1.606369, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.640000, Loss: 1.761685, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.640000, Loss: 3.100976, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.680000, Loss: 1.498750, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.760000, Loss: 3.389935, Validation accuracy: 0.413333\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.720000, Loss: 0.929774, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.640000, Loss: 0.799788, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.720000, Loss: 2.415680, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.720000, Loss: 2.426637, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.800000, Loss: 3.121049, Validation accuracy: 0.426667\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.720000, Loss: 1.586118, Validation accuracy: 0.440000\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.840000, Loss: 1.451298, Validation accuracy: 0.440000\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.680000, Loss: 1.299367, Validation accuracy: 0.466667\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.680000, Loss: 3.274558, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.640000, Loss: 3.223353, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.880000, Loss: 2.510204, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.800000, Loss: 2.681233, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n",
      "EuroSAT model 3/5 results: 0.653\n",
      "Training EuroSAT model 4/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.280000, Loss: 1.771935, Validation accuracy: 0.093333\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.160000, Loss: 2.636596, Validation accuracy: 0.200000\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.280000, Loss: 2.494381, Validation accuracy: 0.186667\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.320000, Loss: 0.992373, Validation accuracy: 0.186667\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.320000, Loss: 0.225468, Validation accuracy: 0.213333\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.320000, Loss: 2.844488, Validation accuracy: 0.200000\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.320000, Loss: 3.599508, Validation accuracy: 0.213333\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.360000, Loss: 0.575536, Validation accuracy: 0.226667\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.480000, Loss: 4.262878, Validation accuracy: 0.266667\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.360000, Loss: 3.337966, Validation accuracy: 0.226667\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.360000, Loss: 1.526746, Validation accuracy: 0.346667\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.520000, Loss: 3.554988, Validation accuracy: 0.293333\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.320000, Loss: 0.937547, Validation accuracy: 0.320000\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.520000, Loss: 0.899174, Validation accuracy: 0.306667\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.520000, Loss: 2.476488, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.640000, Loss: 1.338259, Validation accuracy: 0.426667\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.680000, Loss: 2.349952, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.720000, Loss: 2.723776, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.680000, Loss: 1.500376, Validation accuracy: 0.453333\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.600000, Loss: 2.480923, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.600000, Loss: 2.955485, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.880000, Loss: 1.032891, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.560000, Loss: 1.109147, Validation accuracy: 0.413333\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.640000, Loss: 3.225420, Validation accuracy: 0.373333\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.640000, Loss: 0.658093, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.720000, Loss: 2.761034, Validation accuracy: 0.613333\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.760000, Loss: 1.651032, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.760000, Loss: 0.805182, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.720000, Loss: 4.172180, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.680000, Loss: 3.202985, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "EuroSAT model 4/5 results: 0.547\n",
      "Training EuroSAT model 5/5\n",
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/30, Training accuracy: 0.360000, Loss: 1.927906, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n",
      "Epoch: 2/30, Training accuracy: 0.360000, Loss: 2.468210, Validation accuracy: 0.373333\n",
      "--------------------------------------------------\n",
      "Epoch: 3/30, Training accuracy: 0.280000, Loss: 2.553800, Validation accuracy: 0.453333\n",
      "--------------------------------------------------\n",
      "Epoch: 4/30, Training accuracy: 0.440000, Loss: 4.138227, Validation accuracy: 0.440000\n",
      "--------------------------------------------------\n",
      "Epoch: 5/30, Training accuracy: 0.480000, Loss: 2.135035, Validation accuracy: 0.466667\n",
      "--------------------------------------------------\n",
      "Epoch: 6/30, Training accuracy: 0.400000, Loss: 1.157072, Validation accuracy: 0.480000\n",
      "--------------------------------------------------\n",
      "Epoch: 7/30, Training accuracy: 0.520000, Loss: 0.451131, Validation accuracy: 0.360000\n",
      "--------------------------------------------------\n",
      "Epoch: 8/30, Training accuracy: 0.480000, Loss: 4.431282, Validation accuracy: 0.293333\n",
      "--------------------------------------------------\n",
      "Epoch: 9/30, Training accuracy: 0.560000, Loss: 2.046889, Validation accuracy: 0.493333\n",
      "--------------------------------------------------\n",
      "Epoch: 10/30, Training accuracy: 0.680000, Loss: 3.093680, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n",
      "Epoch: 11/30, Training accuracy: 0.440000, Loss: 4.560717, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 12/30, Training accuracy: 0.560000, Loss: 0.729612, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 13/30, Training accuracy: 0.520000, Loss: 3.406316, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n",
      "Epoch: 14/30, Training accuracy: 0.640000, Loss: 1.243836, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 15/30, Training accuracy: 0.680000, Loss: 3.483853, Validation accuracy: 0.466667\n",
      "--------------------------------------------------\n",
      "Epoch: 16/30, Training accuracy: 0.720000, Loss: 3.178028, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n",
      "Epoch: 17/30, Training accuracy: 0.720000, Loss: 1.011821, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n",
      "Epoch: 18/30, Training accuracy: 0.880000, Loss: 1.749162, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 19/30, Training accuracy: 0.760000, Loss: 1.409082, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n",
      "Epoch: 20/30, Training accuracy: 0.880000, Loss: 2.531635, Validation accuracy: 0.720000\n",
      "--------------------------------------------------\n",
      "Epoch: 21/30, Training accuracy: 0.720000, Loss: 1.571314, Validation accuracy: 0.720000\n",
      "--------------------------------------------------\n",
      "Epoch: 22/30, Training accuracy: 0.760000, Loss: 1.250599, Validation accuracy: 0.733333\n",
      "--------------------------------------------------\n",
      "Epoch: 23/30, Training accuracy: 0.880000, Loss: 2.674052, Validation accuracy: 0.746667\n",
      "--------------------------------------------------\n",
      "Epoch: 24/30, Training accuracy: 0.760000, Loss: 2.916614, Validation accuracy: 0.773333\n",
      "--------------------------------------------------\n",
      "Epoch: 25/30, Training accuracy: 0.840000, Loss: 1.742420, Validation accuracy: 0.733333\n",
      "--------------------------------------------------\n",
      "Epoch: 26/30, Training accuracy: 0.800000, Loss: 3.558167, Validation accuracy: 0.706667\n",
      "--------------------------------------------------\n",
      "Epoch: 27/30, Training accuracy: 0.800000, Loss: 1.647678, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n",
      "Epoch: 28/30, Training accuracy: 0.680000, Loss: 1.139064, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n",
      "Epoch: 29/30, Training accuracy: 0.760000, Loss: 3.109036, Validation accuracy: 0.506667\n",
      "--------------------------------------------------\n",
      "Epoch: 30/30, Training accuracy: 0.720000, Loss: 3.559531, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n",
      "EuroSAT model 5/5 results: 0.600\n",
      "Average results: 0.555\n"
     ]
    }
   ],
   "source": [
    "# Image Size\n",
    "image_size = 224\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers (id not good, decrease learning rate (try 0.001, 0.0001, 0.00001))\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Weight decay (if not good, increase weight_decay (try 1e-4, 1e-3, 1e-2))\n",
    "weight_decay=1e-2\n",
    "\n",
    "# Define the used model\n",
    "model = model_resnet34\n",
    "model.name = \"resnet34\"     # Set name for saving\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "pretrained_model = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "save_model(pretrained_model)\n",
    "\n",
    "test_acc = evaluate(pretrained_model, test_loader, device, criterion)\n",
    "print(f\"test acc: {test_acc:.3f}\")\n",
    "\n",
    "batch_size = 12\n",
    "\n",
    "# Load the pretrained model\n",
    "model_resnet34_fine = load_model(\"resnet34\")\n",
    "\n",
    "for param in model_resnet34_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet34_fine.fc = nn.Linear(model_resnet34_fine.fc.in_features, 5)\n",
    "\n",
    "# Define loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_resnet34_fine.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Train and Evaluate the model on the EuroSAT dataset\n",
    "i = 5\n",
    "average_results = 0.0\n",
    "\n",
    "for j in range(i):\n",
    "    print(f\"Training EuroSAT model {j+1}/{i}\")\n",
    "    eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "    model_resnet34_fine = train(model_resnet34_fine, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n",
    "    results = evaluate(model_resnet34_fine, eurosat_dataloader_validation, device, criterion)\n",
    "    average_results += results\n",
    "    print(f\"EuroSAT model {j+1}/{i} results: {results:.3f}\")\n",
    "    # Save the results to results.txt\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{model_resnet34_fine.name} {j} Time: {time.strftime('%d/%m/%Y, %H:%M:%S')} Epochs: {num_epochs} | Results: {results:.6f}\\n\")\n",
    "\n",
    "average_results /= i\n",
    "print(f\"Average results: {average_results:.3f}\")\n",
    "with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{model_resnet34_fine.name} Average results: {average_results:.6f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model resnet34\n"
     ]
    }
   ],
   "source": [
    "save_model(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/10, Step: 100/210, Loss: 3.8447\n",
      "Epoch: 1/10, Step: 200/210, Loss: 2.9530\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 12.75 GiB is allocated by PyTorch, and 591.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Define optimizer\u001b[39;00m\n\u001b[0;32m     30\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[1;32m---> 32\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m save_model(pretrained_model)\n\u001b[0;32m     36\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m evaluate(pretrained_model, test_loader, device, criterion)\n",
      "Cell \u001b[1;32mIn[10], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainLoader, valLoader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trainLoader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# # reduce learning rate every 10 epochs by factor of 10\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# if (epoch+1) % 10 == 0:\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#     for param_group in optimizer.param_groups:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m valid_epoch_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m lg\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAIN: loss.item(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_running_correct: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_running_correct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m train_epoch_acc \u001b[38;5;241m=\u001b[39m train_running_correct  \u001b[38;5;241m/\u001b[39m num_images\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, device, criterion)\u001b[0m\n\u001b[0;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the loss.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torchvision\\models\\resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torchvision\\models\\resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[1;32m--> 155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\lib\\site-packages\\torch\\nn\\functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 12.75 GiB is allocated by PyTorch, and 591.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Image Size\n",
    "image_size = 224\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers (id not good, decrease learning rate (try 0.001, 0.0001, 0.00001))\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Weight decay (if not good, increase weight_decay (try 1e-4, 1e-3, 1e-2))\n",
    "weight_decay=1e-2\n",
    "\n",
    "# Define the used model\n",
    "model = model_resnet50\n",
    "model.name = \"resnet50\"     # Set name for saving\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "pretrained_model = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "save_model(pretrained_model)\n",
    "\n",
    "test_acc = evaluate(pretrained_model, test_loader, device, criterion)\n",
    "print(f\"test acc: {test_acc:.3f}\")\n",
    "\n",
    "batch_size = 12\n",
    "\n",
    "# Load the pretrained model\n",
    "model_resnet50_fine = load_model(\"resnet50\")\n",
    "\n",
    "for param in model_resnet50_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet50_fine.fc = nn.Linear(model_resnet50_fine.fc.in_features, 5)\n",
    "\n",
    "# Define loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_resnet50_fine.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Train and Evaluate the model on the EuroSAT dataset\n",
    "i = 5\n",
    "average_results = 0.0\n",
    "\n",
    "for j in range(i):\n",
    "    print(f\"Training EuroSAT model {j+1}/{i}\")\n",
    "    eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "    model_resnet50_fine = train(model_resnet50_fine, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n",
    "    results = evaluate(model_resnet50_fine, eurosat_dataloader_validation, device, criterion)\n",
    "    average_results += results\n",
    "    print(f\"EuroSAT model {j+1}/{i} results: {results:.3f}\")\n",
    "    # Save the results to results.txt\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{model_resnet50_fine.name} {j} Time: {time.strftime('%d/%m/%Y, %H:%M:%S')} Epochs: {num_epochs} | Results: {results:.6f}\\n\")\n",
    "\n",
    "average_results /= i\n",
    "print(f\"Average results: {average_results:.3f}\")\n",
    "with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{model_resnet50_fine.name} Average results: {average_results:.6f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/10, Step: 100/210, Loss: 3.8830\n",
      "Epoch: 1/10, Step: 200/210, Loss: 3.0634\n",
      "Epoch: 1/10, Training accuracy: 0.099665, Loss: 3.104908, Validation accuracy: 0.264410\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Define optimizer\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[1;32m---> 33\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m save_model(pretrained_model)\n\u001b[0;32m     37\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m evaluate(pretrained_model, test_loader, device, criterion)\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainLoader, valLoader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m image, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 23\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Image Size\n",
    "image_size = 224\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers (if not good, decrease learning rate (try 0.001, 0.0001, 0.00001))\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Weight decay (if not good, increase weight_decay (try 1e-4, 1e-3, 1e-2))\n",
    "weight_decay=1e-2\n",
    "\n",
    "# Define the used model\n",
    "model = model_vgg11\n",
    "model.name = \"vgg11\"     # Set name for saving\n",
    "\n",
    "# Change the fc layer\n",
    "# model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "pretrained_model = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "save_model(pretrained_model)\n",
    "\n",
    "test_acc = evaluate(pretrained_model, test_loader, device, criterion)\n",
    "print(f\"test acc: {test_acc:.3f}\")\n",
    "\n",
    "batch_size = 12\n",
    "\n",
    "# Load the pretrained model\n",
    "model_vgg11_fine = load_model(\"vgg11\")\n",
    "\n",
    "for param in model_vgg11_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_vgg11_fine.fc = nn.Linear(model_vgg11_fine.fc.in_features, 5)\n",
    "\n",
    "# Define loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_vgg11_fine.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Train and Evaluate the model on the EuroSAT dataset\n",
    "i = 5\n",
    "average_results = 0.0\n",
    "\n",
    "for j in range(i):\n",
    "    print(f\"Training EuroSAT model {j+1}/{i}\")\n",
    "    eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "    model_vgg11_fine = train(model_vgg11_fine, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n",
    "    results = evaluate(model_vgg11_fine, eurosat_dataloader_validation, device, criterion)\n",
    "    average_results += results\n",
    "    print(f\"EuroSAT model {j+1}/{i} results: {results:.3f}\")\n",
    "    # Save the results to results.txt\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{model_vgg11_fine.name} {j} Time: {time.strftime('%d/%m/%Y, %H:%M:%S')} Epochs: {num_epochs} | Results: {results:.6f}\\n\")\n",
    "\n",
    "average_results /= i\n",
    "print(f\"Average results: {average_results:.3f}\")\n",
    "with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{model_vgg11_fine.name} Average results: {average_results:.6f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET: CUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the validation set: 11788\n",
      "Num classes: 200\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "cub_dataset = torchvision.datasets.ImageFolder(root='./data/CUB_200_2011/images')\n",
    "\n",
    "# Transform functions\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "])\n",
    "\n",
    "cub_dataset.transform = transform_eval\n",
    "\n",
    "# Getting the number of classes\n",
    "cub_num_classes = len(cub_dataset.classes)\n",
    "\n",
    "cub_val_loader = DataLoader(cub_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "# Printing the sizes of the resulting sets\n",
    "print(f\"Number of samples in the validation set: {len(cub_dataset)}\")\n",
    "print(f\"Number of classes in CUB-200: {cub_num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model resnet18\n",
      "CUB model results: 0.008\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model_resnet18_fine = load_model(\"resnet18\")\n",
    "\n",
    "for param in model_resnet18_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet18_fine.fc = nn.Linear(model_resnet18_fine.fc.in_features, cub_num_classes)     # Change the fc layer to reflect the number of classes in CUB dataset\n",
    "criterion = nn.CrossEntropyLoss()       # Define loss function and optimizer for fine-tuning\n",
    "\n",
    "# Uncomment if getting errors\n",
    "# device = torch.device('cpu')\n",
    "# model_resnet18_fine.to(device)\n",
    "\n",
    "if (torch.cuda.is_available() and device.type == 'cuda'):\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "cub_model_acc = evaluate(model_resnet18_fine, cub_val_loader, device, criterion)\n",
    "print(f\"ResNet18 CUB model results: {cub_model_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model_resnet34_fine = load_model(\"resnet34\")\n",
    "\n",
    "for param in model_resnet34_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet34_fine.fc = nn.Linear(model_resnet34_fine.fc.in_features, cub_num_classes)     # Change the fc layer to reflect the number of classes in CUB dataset\n",
    "criterion = nn.CrossEntropyLoss()       # Define loss function and optimizer for fine-tuning\n",
    "\n",
    "# Uncomment if getting errors\n",
    "# device = torch.device('cpu')\n",
    "# model_resnet34_fine.to(device)\n",
    "\n",
    "if (torch.cuda.is_available() and device.type == 'cuda'):\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "cub_model_acc = evaluate(model_resnet34_fine, cub_val_loader, device, criterion)\n",
    "print(f\"ResNet34 CUB model results: {cub_model_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model_resnet50_fine = load_model(\"resnet50\")\n",
    "\n",
    "for param in model_resnet50_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet50_fine.fc = nn.Linear(model_resnet50_fine.fc.in_features, cub_num_classes)     # Change the fc layer to reflect the number of classes in CUB dataset\n",
    "criterion = nn.CrossEntropyLoss()       # Define loss function and optimizer for fine-tuning\n",
    "\n",
    "# Uncomment if getting errors\n",
    "# device = torch.device('cpu')\n",
    "# model_resnet50_fine.to(device)\n",
    "\n",
    "if (torch.cuda.is_available() and device.type == 'cuda'):\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "cub_model_acc = evaluate(model_resnet50_fine, cub_val_loader, device, criterion)\n",
    "print(f\"ResNet50 CUB model results: {cub_model_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model_vgg11_fine = load_model(\"vgg11\")\n",
    "\n",
    "for param in model_vgg11_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_vgg11_fine.fc = nn.Linear(model_vgg11_fine.fc.in_features, cub_num_classes)     # Change the fc layer to reflect the number of classes in CUB dataset\n",
    "criterion = nn.CrossEntropyLoss()       # Define loss function and optimizer for fine-tuning\n",
    "\n",
    "# Uncomment if getting errors\n",
    "# device = torch.device('cpu')\n",
    "# model_vgg11_fine.to(device)\n",
    "\n",
    "if (torch.cuda.is_available() and device.type == 'cuda'):\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "cub_model_acc = evaluate(model_vgg11_fine, cub_val_loader, device, criterion)\n",
    "print(f\"ResNet50 CUB model results: {cub_model_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
