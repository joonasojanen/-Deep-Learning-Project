{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 521153S Deep Learning Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152, ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights, vgg11, vgg13, vgg16, vgg19, VGG11_Weights, VGG13_Weights, VGG16_Weights, VGG19_Weights\n",
    "import gdown\n",
    "import urllib\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import time\n",
    "import shutil\n",
    "import logging as lg\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logging level\n",
    "# Possible Levels: DEBUG, INFO,\n",
    "lg.basicConfig(level=lg.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# If you encounter some issues regarding cuda device, e.g., \"RuntimeError: CUDA Out of memory error\",\n",
    "# try to switch the device to cpu by using the following code\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Size\n",
    "image_size = 224\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers (id not good, decrease learning rate (try 0.001, 0.0001, 0.00001))\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Weight decay (if not good, increase weight_decay (try 1e-4, 1e-3, 1e-2))\n",
    "weight_decay=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "val_url = 'https://drive.google.com/u/0/uc?id=1hSMUMj5IRpf-nQs1OwgiQLmGZCN0KDWl'\n",
    "train_url = 'https://drive.google.com/u/0/uc?id=107FTosYIeBn5QbynR46YG91nHcJ70whs'\n",
    "test_url = 'https://drive.google.com/u/0/uc?id=1yKyKgxcnGMIAnA_6Vr2ilbpHMc9COg-v'\n",
    "eurosat_url = 'https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1'\n",
    "cub_url = 'https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1'\n",
    "\n",
    "val_file = './data/val.tar'\n",
    "train_file = './data/train.tar'\n",
    "test_file = './data/test.tar'\n",
    "eurosat_file = './data/eurosatrgb.zip'\n",
    "cub_file = './data/cub_200_2011.tgz'\n",
    "\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    print('Creating data directory')\n",
    "    os.mkdir('./data')\n",
    "\n",
    "if not os.path.exists('./data/val.tar'):\n",
    "    print('Downloading val.tar')\n",
    "    gdown.download(val_url, val_file)\n",
    "    val_tar = tarfile.open(val_file)\n",
    "    val_tar.extractall('./data/')\n",
    "    val_tar.close()\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print('Downloading train.tar')\n",
    "    gdown.download(train_url, train_file)\n",
    "    train_tar = tarfile.open(train_file)\n",
    "    train_tar.extractall('./data/')\n",
    "    train_tar.close()\n",
    "\n",
    "if not os.path.exists(test_file):\n",
    "    print('Downloading test.tar')\n",
    "    gdown.download(test_url, test_file)\n",
    "    test_tar = tarfile.open(test_file)\n",
    "    test_tar.extractall('./data/')\n",
    "    test_tar.close()\n",
    "\n",
    "if not os.path.exists(eurosat_file):\n",
    "    print('Downloading EuroSAT_RGB.zip')\n",
    "    response = urllib.request.urlretrieve(eurosat_url, eurosat_file)\n",
    "    eurosat_zip = zipfile.ZipFile(eurosat_file)\n",
    "    eurosat_zip.extractall('./data/eurosat')\n",
    "    eurosat_zip.close()\n",
    "\n",
    "if not os.path.exists(cub_file):\n",
    "    print('Downloading cub200_2011.tgz')\n",
    "    response = urllib.request.urlretrieve(cub_url, cub_file)\n",
    "    tar = tarfile.open(cub_file, 'r')\n",
    "    for item in tar:\n",
    "        tar.extract(item, './data')\n",
    "    tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model_resnet34 = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "model_resnet50 = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model_resnet101 = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "model_resnet152 = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "\n",
    "model_vgg11 = vgg11(weights=VGG11_Weights.DEFAULT)\n",
    "model_vgg13 = vgg13(weights=VGG13_Weights.DEFAULT)\n",
    "model_vgg16 = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "model_vgg19 = vgg19(weights=VGG19_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/train',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/test',\n",
    "    transform=transforms.Compose([\n",
    "        #transforms.Resize(image_size),\n",
    "        #transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "validation_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/val',\n",
    "    transform=transforms.Compose([\n",
    "        #transforms.Resize(image_size),\n",
    "        #transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "dataloader_validation = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 26880\n",
      "Number of samples in the validation set: 5760\n",
      "Number of samples in the test set: 5760\n"
     ]
    }
   ],
   "source": [
    "# Stratified split, tries to maintain a proportional distribution of samples for each class in all three sets (train, val and test).\n",
    "\n",
    "# Val and Test ratio total, Train ratio (1-test_ratio)\n",
    "test_ratio = 0.3\n",
    "\n",
    "# Split between Val and Test\n",
    "test_val_split = 0.5\n",
    "\n",
    "# Loading the ImageFolder dataset\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='./data/train')\n",
    "\n",
    "# Getting the number of classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Initializing lists to store indices for train, validation, and test sets\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "\n",
    "# Splitting each class separately\n",
    "for class_index in range(num_classes):\n",
    "    # Getting indices for samples belonging to the current class\n",
    "    class_indices = np.where(np.array(train_dataset.targets) == class_index)[0]\n",
    "    \n",
    "    # Splitting indices for each class separately into train, validation, and test sets\n",
    "    train_idx, temp_idx = train_test_split(class_indices, test_size=test_ratio, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=test_val_split, random_state=42)\n",
    "    \n",
    "    # Extending the lists to store indices for each set\n",
    "    train_indices.extend(train_idx)\n",
    "    val_indices.extend(val_idx)\n",
    "    test_indices.extend(test_idx)\n",
    "\n",
    "# Creating Subset objects using the selected indices for each set\n",
    "train_set = Subset(train_dataset, train_indices)\n",
    "val_set = Subset(train_dataset, val_indices)\n",
    "test_set = Subset(train_dataset, test_indices)\n",
    "\n",
    "# Transform functions\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(image_size),      \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "])\n",
    "\n",
    "train_set.dataset.transform = transform_train\n",
    "val_set.dataset.transform = transform_eval\n",
    "test_set.dataset.transform = transform_eval\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Printing the sizes of the resulting sets\n",
    "print(f\"Number of samples in the training set: {len(train_set)}\")\n",
    "print(f\"Number of samples in the validation set: {len(val_set)}\")\n",
    "print(f\"Number of samples in the test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the used model\n",
    "model = model_resnet18\n",
    "model.name = \"resnet18\"     # Set name for saving\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, criterion):\n",
    "\n",
    "    model.eval() \n",
    "    # valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    num_images = 0.0 # used to accumulate number of images\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        counter += 1\n",
    "        \n",
    "        image, labels = data\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass.\n",
    "        outputs = model(image)\n",
    "        # Calculate the loss.\n",
    "        loss = criterion(outputs, labels)\n",
    "        # valid_running_loss += loss.item()\n",
    "        # Calculate the accuracy.\n",
    "        #preds = outputs.argmax(dim=1)\n",
    "        preds = torch.max(outputs, 1).indices\n",
    "        \n",
    "        #valid_running_correct += preds.eq(labels).sum()\n",
    "        valid_running_correct += torch.sum(preds == labels.data)\n",
    "        num_images += len(labels)\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    # epoch_loss = valid_running_loss / counter\n",
    "    lg.debug(f\"EVAL:   loss.item(): {loss.item()} valid_running_correct: {valid_running_correct.item()}\")\n",
    "\n",
    "    epoch_acc = valid_running_correct / num_images\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainLoader, valLoader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Start the training.\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA\")\n",
    "\n",
    "    print('Training')\n",
    "    \n",
    "    lg.debug(f\" len(trainLoader.dataset): {len(trainLoader.dataset)}\")\n",
    "    lg.debug(f\" len(valLoader.dataset): {len(valLoader.dataset)}\")\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_running_loss = 0.0\n",
    "        train_running_correct = 0\n",
    "        counter = 0\n",
    "        num_images = 0.0 # used to accumulate number of images\n",
    "        for i, data in enumerate(trainLoader,0):\n",
    "            counter += 1\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            train_running_correct += torch.sum(preds == labels.data)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            # Update the weights.\n",
    "            optimizer.step()    # Output training \n",
    "            num_images += len(image)\n",
    "            if counter % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}/{num_epochs}, Step: {counter}/{len(trainLoader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # reduce learning rate every 10 epochs by factor of 10\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 10.0\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        valid_epoch_acc = evaluate(model, valLoader, device, criterion)\n",
    "        lg.debug(f\"TRAIN: loss.item(): {loss.item()}, train_running_correct: {train_running_correct}\")\n",
    "        \n",
    "        # train_epoch_loss = train_running_loss / counter\n",
    "        # train_loss.append(train_epoch_loss)\n",
    "        # valid_loss.append(valid_epoch_loss)\n",
    "        train_epoch_acc = train_running_correct  / num_images\n",
    "        # train_acc.append(train_epoch_acc )\n",
    "\n",
    "        print('Epoch: %d/%d, Training accuracy: %f, Loss: %f, Validation accuracy: %f' % (epoch+1, num_epochs, train_epoch_acc, loss.item(), valid_epoch_acc))\n",
    "\n",
    "        # valid_acc.append(valid_epoch_acc)\n",
    "        # lg.debug(f\"Training loss mean: {train_epoch_loss :.3f}\")\n",
    "        # print(f\"Validation acc: {valid_epoch_acc:.3f}\")\n",
    "        print('-'*50)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = train(model, dataloader_train, dataloader_validation, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root: len(trainLoader.dataset): 26880\n",
      "DEBUG:root: len(valLoader.dataset): 5760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training\n",
      "Epoch: 1/10, Step: 100/210, Loss: 2.8310\n",
      "Epoch: 1/10, Step: 200/210, Loss: 1.7128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 2.3583860397338867 valid_running_correct: 4034\n",
      "DEBUG:root:TRAIN: loss.item(): 1.6559829711914062, train_running_correct: 11243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Training accuracy: 0.418266, Loss: 1.655983, Validation accuracy: 0.700347\n",
      "--------------------------------------------------\n",
      "Epoch: 2/10, Step: 100/210, Loss: 1.1455\n",
      "Epoch: 2/10, Step: 200/210, Loss: 0.9268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 1.2889606952667236 valid_running_correct: 4510\n",
      "DEBUG:root:TRAIN: loss.item(): 1.123974084854126, train_running_correct: 20419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10, Training accuracy: 0.759635, Loss: 1.123974, Validation accuracy: 0.782986\n",
      "--------------------------------------------------\n",
      "Epoch: 3/10, Step: 100/210, Loss: 0.7966\n",
      "Epoch: 3/10, Step: 200/210, Loss: 0.7529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 1.0130066871643066 valid_running_correct: 4664\n",
      "DEBUG:root:TRAIN: loss.item(): 0.6720293164253235, train_running_correct: 22036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10, Training accuracy: 0.819792, Loss: 0.672029, Validation accuracy: 0.809722\n",
      "--------------------------------------------------\n",
      "Epoch: 4/10, Step: 100/210, Loss: 0.5519\n",
      "Epoch: 4/10, Step: 200/210, Loss: 0.6234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.9021157026290894 valid_running_correct: 4742\n",
      "DEBUG:root:TRAIN: loss.item(): 0.5895287990570068, train_running_correct: 22950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10, Training accuracy: 0.853795, Loss: 0.589529, Validation accuracy: 0.823264\n",
      "--------------------------------------------------\n",
      "Epoch: 5/10, Step: 100/210, Loss: 0.5154\n",
      "Epoch: 5/10, Step: 200/210, Loss: 0.5028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.8686224222183228 valid_running_correct: 4800\n",
      "DEBUG:root:TRAIN: loss.item(): 0.5199425220489502, train_running_correct: 23616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10, Training accuracy: 0.878571, Loss: 0.519943, Validation accuracy: 0.833333\n",
      "--------------------------------------------------\n",
      "Epoch: 6/10, Step: 100/210, Loss: 0.4681\n",
      "Epoch: 6/10, Step: 200/210, Loss: 0.4194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.804081380367279 valid_running_correct: 4822\n",
      "DEBUG:root:TRAIN: loss.item(): 0.46084141731262207, train_running_correct: 24227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10, Training accuracy: 0.901302, Loss: 0.460841, Validation accuracy: 0.837153\n",
      "--------------------------------------------------\n",
      "Epoch: 7/10, Step: 100/210, Loss: 0.3703\n",
      "Epoch: 7/10, Step: 200/210, Loss: 0.3078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.7402129173278809 valid_running_correct: 4840\n",
      "DEBUG:root:TRAIN: loss.item(): 0.32397887110710144, train_running_correct: 24670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10, Training accuracy: 0.917783, Loss: 0.323979, Validation accuracy: 0.840278\n",
      "--------------------------------------------------\n",
      "Epoch: 8/10, Step: 100/210, Loss: 0.2849\n",
      "Epoch: 8/10, Step: 200/210, Loss: 0.2818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.7276484966278076 valid_running_correct: 4879\n",
      "DEBUG:root:TRAIN: loss.item(): 0.2579147219657898, train_running_correct: 25127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10, Training accuracy: 0.934784, Loss: 0.257915, Validation accuracy: 0.847049\n",
      "--------------------------------------------------\n",
      "Epoch: 9/10, Step: 100/210, Loss: 0.2704\n",
      "Epoch: 9/10, Step: 200/210, Loss: 0.3734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.6640430688858032 valid_running_correct: 4898\n",
      "DEBUG:root:TRAIN: loss.item(): 0.16336844861507416, train_running_correct: 25488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10, Training accuracy: 0.948214, Loss: 0.163368, Validation accuracy: 0.850347\n",
      "--------------------------------------------------\n",
      "Epoch: 10/10, Step: 100/210, Loss: 0.1529\n",
      "Epoch: 10/10, Step: 200/210, Loss: 0.2073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.6935462951660156 valid_running_correct: 4902\n",
      "DEBUG:root:TRAIN: loss.item(): 0.19226767122745514, train_running_correct: 25786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10, Training accuracy: 0.959301, Loss: 0.192268, Validation accuracy: 0.851042\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(trained_model):\n",
    "    # Create Models Folder\n",
    "    if not os.path.exists('./models'):\n",
    "        print('Creating models directory')\n",
    "        os.mkdir('./models')\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists(f'./models/{trained_model.name}.pth'):\n",
    "        print(f'Saving Model {trained_model.name}')\n",
    "        torch.save(trained_model, f'./models/{trained_model.name}.pth')\n",
    "\n",
    "def load_model(model_name):\n",
    "    if not os.path.exists(f'./models/{model_name}.pth'):\n",
    "        print(f'Cannot Load {model_name}')\n",
    "        return None\n",
    "    print(f'Loading Model {model_name}')\n",
    "    model = torch.load(f'./models/{model_name}.pth')\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.8225520849227905 valid_running_correct: 4919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.854\n"
     ]
    }
   ],
   "source": [
    "# validate_loss, validate_acc = evaluate(trained_model, dataloader_validation, criterion, device)\n",
    "# print(f\"Validation loss: {validate_loss:.3f}, validation acc: {validate_acc:.3f}\")\n",
    "test_acc = evaluate(pretrained_model, test_loader, device, criterion)\n",
    "print(f\"test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_100_eurosat():\n",
    "    '''\n",
    "    The `choose_100_eurosat()` function selects 100 images from the EuroSAT dataset and copies them to the `data/eurosat_validation` folder. It also creates a `data/eurosat_training` folder if it doesn't already exist.\n",
    "    '''\n",
    "    if not os.path.exists('./data/eurosat_validation'):\n",
    "        print('Creating data directory')\n",
    "        os.mkdir('./data/eurosat_validation')\n",
    "    else:\n",
    "        # Folder full of images -> delete all files in the directory\n",
    "        shutil.rmtree(os.path.join('./data/eurosat_validation'))\n",
    "        os.mkdir('./data/eurosat_validation')\n",
    "\n",
    "    if not os.path.exists('./data/eurosat_training'):\n",
    "        print('Creating training directory')\n",
    "        os.mkdir('./data/eurosat_training')\n",
    "    else:\n",
    "        # Folder full of images -> delete all files in the directory\n",
    "        shutil.rmtree(os.path.join('./data/eurosat_training'))\n",
    "        os.mkdir('./data/eurosat_training')\n",
    "\n",
    "    # Load the EuroSAT Categories\n",
    "    eurosat_categories = [name for name in os.listdir('./data/eurosat/EuroSAT_RGB/') if os.path.isdir(os.path.join('./data/eurosat/EuroSAT_RGB/', name))]\n",
    "    # Randomly select 5 categories\n",
    "    lg.debug(f\"Selecting 5 categories from {eurosat_categories} categories\")\n",
    "    selected_categories = np.random.choice(eurosat_categories, 5, replace=False)\n",
    "    lg.debug(f\"Selected categories: {selected_categories}\")\n",
    "\n",
    "    selected_images = []\n",
    "    training_images = []\n",
    "\n",
    "    # From each directory, randomly select 20 images\n",
    "    for category in selected_categories:\n",
    "        images = os.listdir(os.path.join('./data/eurosat/EuroSAT_RGB/', category))\n",
    "        selected = random.sample(images, 20)\n",
    "        selected_images.extend([(category, image) for image in selected])\n",
    "        lg.debug(f\"Selected {selected} from {category}\")\n",
    "        # Copy selected images to the selected directory\n",
    "        for image in selected:\n",
    "            if not os.path.exists(f\"./data/eurosat_validation/{category}\"):\n",
    "                lg.debug(f\"Creating {category} directory\")\n",
    "                os.mkdir(f\"./data/eurosat_validation/{category}\")\n",
    "            shutil.copyfile(os.path.join('./data/eurosat/EuroSAT_RGB', category, image), os.path.join(f\"./data/eurosat_validation/{category}\", image))\n",
    "\n",
    "    # From these 100 images, randomly select 5 images from each category for the training set\n",
    "    for category in selected_categories:\n",
    "        category_images = [image for (cat, image) in selected_images if cat == category]\n",
    "        training = random.sample(category_images, 5)\n",
    "        training_images.extend(training)\n",
    "        lg.debug(f\"Selected {training} for training\")\n",
    "\n",
    "        # Copy training images to the training directory\n",
    "        for image in training:\n",
    "            if not os.path.exists(f\"./data/eurosat_training/{category}\"):\n",
    "                lg.debug(f\"Creating {category} directory\")\n",
    "                os.mkdir(f\"./data/eurosat_training/{category}\")\n",
    "            shutil.move(os.path.join(f\"./data/eurosat_validation/{category}\", image), os.path.join(f\"./data/eurosat_training/{category}\", image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_euro_datasets():\n",
    "\n",
    "    # Choose 100 images from EuroSAT dataset\n",
    "    choose_100_eurosat()\n",
    "\n",
    "    eurosat_train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root='./data/eurosat_training',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    eurosat_validation_dataset = torchvision.datasets.ImageFolder(\n",
    "        root='./data/eurosat_validation',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    eurosat_dataloader_train = DataLoader(eurosat_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    eurosat_dataloader_validation = DataLoader(eurosat_validation_dataset, batch_size=batch_size, shuffle = False, num_workers=num_workers)\n",
    "\n",
    "    return eurosat_dataloader_train, eurosat_dataloader_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Selecting 5 categories from ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake'] categories\n",
      "DEBUG:root:Selected categories: ['Residential' 'PermanentCrop' 'HerbaceousVegetation' 'River' 'SeaLake']\n",
      "DEBUG:root:Selected ['Residential_1541.jpg', 'Residential_40.jpg', 'Residential_158.jpg', 'Residential_1945.jpg', 'Residential_1617.jpg', 'Residential_1105.jpg', 'Residential_66.jpg', 'Residential_193.jpg', 'Residential_424.jpg', 'Residential_1889.jpg', 'Residential_2951.jpg', 'Residential_2.jpg', 'Residential_2690.jpg', 'Residential_423.jpg', 'Residential_1132.jpg', 'Residential_2785.jpg', 'Residential_1381.jpg', 'Residential_1046.jpg', 'Residential_1532.jpg', 'Residential_2700.jpg'] from Residential\n",
      "DEBUG:root:Creating Residential directory\n",
      "DEBUG:root:Selected ['PermanentCrop_287.jpg', 'PermanentCrop_2308.jpg', 'PermanentCrop_715.jpg', 'PermanentCrop_290.jpg', 'PermanentCrop_702.jpg', 'PermanentCrop_60.jpg', 'PermanentCrop_1884.jpg', 'PermanentCrop_1201.jpg', 'PermanentCrop_2029.jpg', 'PermanentCrop_1352.jpg', 'PermanentCrop_2275.jpg', 'PermanentCrop_333.jpg', 'PermanentCrop_98.jpg', 'PermanentCrop_1736.jpg', 'PermanentCrop_2019.jpg', 'PermanentCrop_1831.jpg', 'PermanentCrop_923.jpg', 'PermanentCrop_992.jpg', 'PermanentCrop_2328.jpg', 'PermanentCrop_1452.jpg'] from PermanentCrop\n",
      "DEBUG:root:Creating PermanentCrop directory\n",
      "DEBUG:root:Selected ['HerbaceousVegetation_632.jpg', 'HerbaceousVegetation_1497.jpg', 'HerbaceousVegetation_974.jpg', 'HerbaceousVegetation_639.jpg', 'HerbaceousVegetation_2129.jpg', 'HerbaceousVegetation_2075.jpg', 'HerbaceousVegetation_2998.jpg', 'HerbaceousVegetation_2419.jpg', 'HerbaceousVegetation_1463.jpg', 'HerbaceousVegetation_2625.jpg', 'HerbaceousVegetation_2536.jpg', 'HerbaceousVegetation_2418.jpg', 'HerbaceousVegetation_2253.jpg', 'HerbaceousVegetation_22.jpg', 'HerbaceousVegetation_904.jpg', 'HerbaceousVegetation_1682.jpg', 'HerbaceousVegetation_2939.jpg', 'HerbaceousVegetation_2738.jpg', 'HerbaceousVegetation_1787.jpg', 'HerbaceousVegetation_2995.jpg'] from HerbaceousVegetation\n",
      "DEBUG:root:Creating HerbaceousVegetation directory\n",
      "DEBUG:root:Selected ['River_20.jpg', 'River_448.jpg', 'River_109.jpg', 'River_715.jpg', 'River_2202.jpg', 'River_1742.jpg', 'River_1800.jpg', 'River_1988.jpg', 'River_293.jpg', 'River_537.jpg', 'River_2071.jpg', 'River_2243.jpg', 'River_2296.jpg', 'River_940.jpg', 'River_470.jpg', 'River_157.jpg', 'River_472.jpg', 'River_1127.jpg', 'River_356.jpg', 'River_1864.jpg'] from River\n",
      "DEBUG:root:Creating River directory\n",
      "DEBUG:root:Selected ['SeaLake_458.jpg', 'SeaLake_2764.jpg', 'SeaLake_1031.jpg', 'SeaLake_2735.jpg', 'SeaLake_2392.jpg', 'SeaLake_785.jpg', 'SeaLake_613.jpg', 'SeaLake_590.jpg', 'SeaLake_1407.jpg', 'SeaLake_2600.jpg', 'SeaLake_1007.jpg', 'SeaLake_80.jpg', 'SeaLake_1387.jpg', 'SeaLake_827.jpg', 'SeaLake_998.jpg', 'SeaLake_362.jpg', 'SeaLake_977.jpg', 'SeaLake_378.jpg', 'SeaLake_1570.jpg', 'SeaLake_104.jpg'] from SeaLake\n",
      "DEBUG:root:Creating SeaLake directory\n",
      "DEBUG:root:Selected ['Residential_2690.jpg', 'Residential_1945.jpg', 'Residential_1532.jpg', 'Residential_1889.jpg', 'Residential_423.jpg'] for training\n",
      "DEBUG:root:Creating Residential directory\n",
      "DEBUG:root:Selected ['PermanentCrop_333.jpg', 'PermanentCrop_2308.jpg', 'PermanentCrop_2029.jpg', 'PermanentCrop_992.jpg', 'PermanentCrop_1452.jpg'] for training\n",
      "DEBUG:root:Creating PermanentCrop directory\n",
      "DEBUG:root:Selected ['HerbaceousVegetation_1497.jpg', 'HerbaceousVegetation_2419.jpg', 'HerbaceousVegetation_2075.jpg', 'HerbaceousVegetation_2129.jpg', 'HerbaceousVegetation_904.jpg'] for training\n",
      "DEBUG:root:Creating HerbaceousVegetation directory\n",
      "DEBUG:root:Selected ['River_2243.jpg', 'River_940.jpg', 'River_293.jpg', 'River_715.jpg', 'River_448.jpg'] for training\n",
      "DEBUG:root:Creating River directory\n",
      "DEBUG:root:Selected ['SeaLake_785.jpg', 'SeaLake_1407.jpg', 'SeaLake_1031.jpg', 'SeaLake_1387.jpg', 'SeaLake_80.jpg'] for training\n",
      "DEBUG:root:Creating SeaLake directory\n",
      "DEBUG:root: len(trainLoader.dataset): 25\n",
      "DEBUG:root: len(valLoader.dataset): 75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model resnet18\n",
      "Using CUDA\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 1.6423953771591187 valid_running_correct: 14\n",
      "DEBUG:root:TRAIN: loss.item(): 2.4552183151245117, train_running_correct: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30, Training accuracy: 0.200000, Loss: 2.455218, Validation accuracy: 0.186667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 1.841840386390686 valid_running_correct: 15\n",
      "DEBUG:root:TRAIN: loss.item(): 1.1028521060943604, train_running_correct: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/30, Training accuracy: 0.240000, Loss: 1.102852, Validation accuracy: 0.200000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 1.8407410383224487 valid_running_correct: 15\n",
      "DEBUG:root:TRAIN: loss.item(): 2.312901258468628, train_running_correct: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/30, Training accuracy: 0.200000, Loss: 2.312901, Validation accuracy: 0.200000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 1.591775894165039 valid_running_correct: 22\n",
      "DEBUG:root:TRAIN: loss.item(): 0.9199113845825195, train_running_correct: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/30, Training accuracy: 0.200000, Loss: 0.919911, Validation accuracy: 0.293333\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 1.3473542928695679 valid_running_correct: 35\n",
      "DEBUG:root:TRAIN: loss.item(): 1.3981192111968994, train_running_correct: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/30, Training accuracy: 0.320000, Loss: 1.398119, Validation accuracy: 0.466667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 1.1076643466949463 valid_running_correct: 40\n",
      "DEBUG:root:TRAIN: loss.item(): 1.445085048675537, train_running_correct: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/30, Training accuracy: 0.320000, Loss: 1.445085, Validation accuracy: 0.533333\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.8751246333122253 valid_running_correct: 42\n",
      "DEBUG:root:TRAIN: loss.item(): 1.721306324005127, train_running_correct: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/30, Training accuracy: 0.640000, Loss: 1.721306, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.7203018665313721 valid_running_correct: 31\n",
      "DEBUG:root:TRAIN: loss.item(): 2.505167245864868, train_running_correct: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/30, Training accuracy: 0.760000, Loss: 2.505167, Validation accuracy: 0.413333\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.3292461931705475 valid_running_correct: 33\n",
      "DEBUG:root:TRAIN: loss.item(): 0.7796617150306702, train_running_correct: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30, Training accuracy: 0.520000, Loss: 0.779662, Validation accuracy: 0.440000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.2577981650829315 valid_running_correct: 30\n",
      "DEBUG:root:TRAIN: loss.item(): 2.8000028133392334, train_running_correct: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30, Training accuracy: 0.600000, Loss: 2.800003, Validation accuracy: 0.400000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.3416111469268799 valid_running_correct: 50\n",
      "DEBUG:root:TRAIN: loss.item(): 2.288508415222168, train_running_correct: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/30, Training accuracy: 0.760000, Loss: 2.288508, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.4253270626068115 valid_running_correct: 54\n",
      "DEBUG:root:TRAIN: loss.item(): 1.2741020917892456, train_running_correct: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/30, Training accuracy: 0.640000, Loss: 1.274102, Validation accuracy: 0.720000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.6885266900062561 valid_running_correct: 44\n",
      "DEBUG:root:TRAIN: loss.item(): 2.700993061065674, train_running_correct: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/30, Training accuracy: 0.640000, Loss: 2.700993, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.7035146355628967 valid_running_correct: 44\n",
      "DEBUG:root:TRAIN: loss.item(): 0.6270663738250732, train_running_correct: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/30, Training accuracy: 0.720000, Loss: 0.627066, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.6168118119239807 valid_running_correct: 42\n",
      "DEBUG:root:TRAIN: loss.item(): 3.08608078956604, train_running_correct: 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/30, Training accuracy: 0.680000, Loss: 3.086081, Validation accuracy: 0.560000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.5353259444236755 valid_running_correct: 45\n",
      "DEBUG:root:TRAIN: loss.item(): 1.5930731296539307, train_running_correct: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/30, Training accuracy: 0.720000, Loss: 1.593073, Validation accuracy: 0.600000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.4178647994995117 valid_running_correct: 41\n",
      "DEBUG:root:TRAIN: loss.item(): 1.4918522834777832, train_running_correct: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/30, Training accuracy: 0.760000, Loss: 1.491852, Validation accuracy: 0.546667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.30357229709625244 valid_running_correct: 50\n",
      "DEBUG:root:TRAIN: loss.item(): 3.5779361724853516, train_running_correct: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/30, Training accuracy: 0.840000, Loss: 3.577936, Validation accuracy: 0.666667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.095213882625103 valid_running_correct: 47\n",
      "DEBUG:root:TRAIN: loss.item(): 0.7022126317024231, train_running_correct: 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/30, Training accuracy: 0.680000, Loss: 0.702213, Validation accuracy: 0.626667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.05948719382286072 valid_running_correct: 44\n",
      "DEBUG:root:TRAIN: loss.item(): 0.7382724285125732, train_running_correct: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/30, Training accuracy: 0.720000, Loss: 0.738272, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.07121492177248001 valid_running_correct: 39\n",
      "DEBUG:root:TRAIN: loss.item(): 1.4224555492401123, train_running_correct: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/30, Training accuracy: 0.560000, Loss: 1.422456, Validation accuracy: 0.520000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.05676199495792389 valid_running_correct: 43\n",
      "DEBUG:root:TRAIN: loss.item(): 2.899841070175171, train_running_correct: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/30, Training accuracy: 0.840000, Loss: 2.899841, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.11926469206809998 valid_running_correct: 49\n",
      "DEBUG:root:TRAIN: loss.item(): 1.0115563869476318, train_running_correct: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/30, Training accuracy: 0.960000, Loss: 1.011556, Validation accuracy: 0.653333\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.1395360231399536 valid_running_correct: 43\n",
      "DEBUG:root:TRAIN: loss.item(): 2.3395068645477295, train_running_correct: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/30, Training accuracy: 0.800000, Loss: 2.339507, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.19262272119522095 valid_running_correct: 43\n",
      "DEBUG:root:TRAIN: loss.item(): 1.0440404415130615, train_running_correct: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/30, Training accuracy: 0.880000, Loss: 1.044040, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.3005083501338959 valid_running_correct: 44\n",
      "DEBUG:root:TRAIN: loss.item(): 3.4589810371398926, train_running_correct: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/30, Training accuracy: 0.880000, Loss: 3.458981, Validation accuracy: 0.586667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.39830002188682556 valid_running_correct: 57\n",
      "DEBUG:root:TRAIN: loss.item(): 2.0505309104919434, train_running_correct: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/30, Training accuracy: 0.920000, Loss: 2.050531, Validation accuracy: 0.760000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.3025105893611908 valid_running_correct: 57\n",
      "DEBUG:root:TRAIN: loss.item(): 3.524022102355957, train_running_correct: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/30, Training accuracy: 0.920000, Loss: 3.524022, Validation accuracy: 0.760000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.1097554937005043 valid_running_correct: 52\n",
      "DEBUG:root:TRAIN: loss.item(): 2.8431854248046875, train_running_correct: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/30, Training accuracy: 0.880000, Loss: 2.843185, Validation accuracy: 0.693333\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:EVAL:   loss.item(): 0.041999027132987976 valid_running_correct: 43\n",
      "DEBUG:root:TRAIN: loss.item(): 3.1651673316955566, train_running_correct: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/30, Training accuracy: 0.760000, Loss: 3.165167, Validation accuracy: 0.573333\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model_resnet18_fine = load_model(\"resnet18\")\n",
    "\n",
    "for param in model_resnet18_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet18_fine.fc = nn.Linear(model_resnet18_fine.fc.in_features, 5)\n",
    "\n",
    "# Define loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_resnet18_fine.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Load the EuroSAT dataset\n",
    "eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "\n",
    "# Train the model on the EuroSAT dataset\n",
    "eurosat_trained_model = train(model_resnet18_fine, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "\n",
    "# Evaluate the model on the EuroSAT dataset\n",
    "i = 10\n",
    "for j in range(i):\n",
    "    print(f\"Training EuroSAT model {j+1}/{i}\")\n",
    "    eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "    eurosat_trained_model = train(eurosat_trained_model, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n",
    "    results = evaluate(eurosat_trained_model, eurosat_dataloader_validation, device, criterion)\n",
    "    print(f\"EuroSAT model {j+1}/{i} results: {results:.3f}\")\n",
    "    # Save the results to results.txt\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{eurosat_trained_model.name} {j} Time: {time.strftime('%d/%m/%Y, %H:%M:%S')} Epochs: {num_epochs} | Results: {results:.6f}\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Size\n",
    "image_size = 224\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers (id not good, decrease learning rate (try 0.001, 0.0001, 0.00001))\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Weight decay (if not good, increase weight_decay (try 1e-4, 1e-3, 1e-2))\n",
    "weight_decay=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the used model\n",
    "model = model_resnet34\n",
    "model.name = \"resnet34\"     # Set name for saving\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_loss, validate_acc = evaluate(trained_model, dataloader_validation, criterion, device)\n",
    "# print(f\"Validation loss: {validate_loss:.3f}, validation acc: {validate_acc:.3f}\")\n",
    "test_acc = evaluate(pretrained_model, test_loader, device, criterion)\n",
    "print(f\"test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model_resnet34_fine = load_model(\"resnet34\")\n",
    "\n",
    "for param in model_resnet34_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet34_fine.fc = nn.Linear(model_resnet34_fine.fc.in_features, 5)\n",
    "\n",
    "# Define loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_resnet34_fine.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Load the EuroSAT dataset\n",
    "eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "\n",
    "eurosat_trained_model = train(model_resnet34_fine, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 11\n",
    "\n",
    "for j in range(i):\n",
    "    print(f\"Training EuroSAT model {j+1}/{i}\")\n",
    "    eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "    eurosat_trained_model = train(eurosat_trained_model, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n",
    "    results = evaluate(eurosat_trained_model, eurosat_dataloader_validation, device, criterion)\n",
    "    print(f\"EuroSAT model {j+1}/{i} results: {results:.3f}\")\n",
    "    # Save the results to results.txt\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{eurosat_trained_model.name} {j} Time: {time.strftime('%d/%m/%Y, %H:%M:%S')} Epochs: {num_epochs} | Results: {results:.6f}\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Size\n",
    "image_size = 224\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers (id not good, decrease learning rate (try 0.001, 0.0001, 0.00001))\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum\n",
    "momentum = 0.9\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Weight decay (if not good, increase weight_decay (try 1e-4, 1e-3, 1e-2))\n",
    "weight_decay=1e-2\n",
    "\n",
    "\n",
    "# Define the used model\n",
    "model = model_resnet50\n",
    "model.name = \"resnet50\"     # Set name for saving\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "pretrained_model = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(pretrained_model)\n",
    "\n",
    "# validate_loss, validate_acc = evaluate(trained_model, dataloader_validation, criterion, device)\n",
    "# print(f\"Validation loss: {validate_loss:.3f}, validation acc: {validate_acc:.3f}\")\n",
    "test_acc = evaluate(pretrained_model, test_loader, device, criterion)\n",
    "print(f\"test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "\n",
    "# Load the pretrained model\n",
    "model_resnet50_fine = load_model(\"resnet50\")\n",
    "\n",
    "for param in model_resnet50_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet50_fine.fc = nn.Linear(model_resnet50_fine.fc.in_features, 5)\n",
    "\n",
    "# Define loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_resnet50_fine.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Load the EuroSAT dataset\n",
    "eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "\n",
    "eurosat_trained_model = train(model_resnet50_fine, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 11\n",
    "j = 1\n",
    "\n",
    "for j in range(i):\n",
    "    print(f\"Training EuroSAT model {j+1}/{i}\")\n",
    "    eurosat_dataloader_train, eurosat_dataloader_validation = load_euro_datasets()\n",
    "    eurosat_trained_model = train(eurosat_trained_model, eurosat_dataloader_train, eurosat_dataloader_validation, criterion, optimizer, num_epochs, device)\n",
    "    results = evaluate(eurosat_trained_model, eurosat_dataloader_validation, device, criterion)\n",
    "    print(f\"EuroSAT model {j+1}/{i} results: {results:.3f}\")\n",
    "    # Save the results to results.txt\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(f\"{j} Time: {time.strftime('%d/%m/%Y, %H:%M:%S')} Epochs: {num_epochs} | Results: {results:.6f}\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET: CUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the validation set: 11788\n"
     ]
    }
   ],
   "source": [
    "cub_dataset = torchvision.datasets.ImageFolder(root='./data/CUB_200_2011/images')\n",
    "\n",
    "# Transform functions\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # mean and std for ImageNet dataset\n",
    "])\n",
    "\n",
    "cub_dataset.transform = transform_eval\n",
    "\n",
    "# Loading the ImageFolder dataset\n",
    "\n",
    "\n",
    "# Getting the number of classes\n",
    "cub_num_classes = len(cub_dataset.classes)\n",
    "\n",
    "cub_val_loader = DataLoader(cub_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "# Printing the sizes of the resulting sets\n",
    "print(f\"Number of samples in the validation set: {len(cub_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model resnet18\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the pretrained model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model_resnet18_fine \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model_resnet18_fine\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m     15\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 17\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading Model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torch\\serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torch\\serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torch\\serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torch\\serialization.py:279\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torch\\_utils.py:117\u001b[0m, in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m     )\n\u001b[1;32m--> 117\u001b[0m     \u001b[43muntyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Batch size during training\n",
    "num_workers = 1\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Weight decay (if not good, increase weight_decay (try 1e-4, 1e-3, 1e-2))\n",
    "weight_decay=1e-2\n",
    "\n",
    "\n",
    "# Load the pretrained model\n",
    "model_resnet18_fine = load_model(\"resnet18\")\n",
    "\n",
    "for param in model_resnet18_fine.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "model_resnet18_fine.fc = nn.Linear(model_resnet18_fine.fc.in_features, 5)\n",
    "\n",
    "# Define loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_resnet18_fine.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model_resnet18_fine.to(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "cub_model = evaluate(model_resnet18_fine, cub_val_loader, device, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
