{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 521153S Deep Learning Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3089096673.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 24\u001b[1;36m\u001b[0m\n\u001b[1;33m    from shutil\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152, ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights\n",
    "import gdown\n",
    "import urllib\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import time\n",
    "from shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download val, test and traininig dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "val_url = 'https://drive.google.com/u/0/uc?id=1hSMUMj5IRpf-nQs1OwgiQLmGZCN0KDWl'\n",
    "train_url = 'https://drive.google.com/u/0/uc?id=107FTosYIeBn5QbynR46YG91nHcJ70whs'\n",
    "test_url = 'https://drive.google.com/u/0/uc?id=1yKyKgxcnGMIAnA_6Vr2ilbpHMc9COg-v'\n",
    "eurosat_url = 'https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1'\n",
    "\n",
    "val_file = './data/val.tar'\n",
    "train_file = './data/train.tar'\n",
    "test_file = './data/test.tar'\n",
    "eurosat_file = './data/eurosatrgb.zip'\n",
    "\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    print('Creating data directory')\n",
    "    os.mkdir('./data')\n",
    "\n",
    "if not os.path.exists('./data/val.tar'):\n",
    "    print('Downloading val.tar')\n",
    "    gdown.download(val_url, val_file)\n",
    "    val_tar = tarfile.open(val_file)\n",
    "    val_tar.extractall('./data/')\n",
    "    val_tar.close()\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print('Downloading train.tar')\n",
    "    gdown.download(train_url, train_file)\n",
    "    train_tar = tarfile.open(train_file)\n",
    "    train_tar.extractall('./data/')\n",
    "    train_tar.close()\n",
    "\n",
    "if not os.path.exists(test_file):\n",
    "    print('Downloading test.tar')\n",
    "    gdown.download(test_url, test_file)\n",
    "    test_tar = tarfile.open(test_file)\n",
    "    test_tar.extractall('./data/')\n",
    "    test_tar.close()\n",
    "\n",
    "if not os.path.exists(eurosat_file):\n",
    "    print('Downloading EuroSAT_RGB.zip')\n",
    "    response = urllib.request.urlretrieve(eurosat_url, eurosat_file)\n",
    "    eurosat_zip = zipfile.ZipFile(eurosat_file)\n",
    "    eurosat_zip.extractall('./data/eurosat')\n",
    "    eurosat_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will switch to cuda device automatically to accelerate your code if gpu is available in your environment.\n",
    "\n",
    "To enable GPU in Google Colab, you can navigate to `Edit → Notebook Settings → Hardware Accelerator` , and then select `T4 GPU` or `TPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# If you encounter some issues regarding cuda device, e.g., \"RuntimeError: CUDA Out of memory error\",\n",
    "# try to switch the device to cpu by using the following code\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Size\n",
    "image_size = 32\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 100\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "model_resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model_resnet34 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "model_resnet50 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model_resnet101 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "model_resnet152 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/train',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/test',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "validation_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/val',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "dataloader_validation = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[Errno 2] No such file or directory: 'test/assets/encode_jpeg/grace_hopper_517x606.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Koodi\\Deep-Learning-Project\\project.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Koodi/Deep-Learning-Project/project.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m img \u001b[39m=\u001b[39m read_image(\u001b[39m\"\u001b[39;49m\u001b[39mtest/assets/encode_jpeg/grace_hopper_517x606.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Koodi/Deep-Learning-Project/project.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Step 1: Initialize model with the best available weights\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Koodi/Deep-Learning-Project/project.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m weights \u001b[39m=\u001b[39m ResNet50_Weights\u001b[39m.\u001b[39mDEFAULT\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\io\\image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m--> 258\u001b[0m data \u001b[39m=\u001b[39m read_file(path)\n\u001b[0;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\io\\image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[1;32m---> 52\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mread_file(path)\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torch\\_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    688\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [Errno 2] No such file or directory: 'test/assets/encode_jpeg/grace_hopper_517x606.jpg'"
     ]
    }
   ],
   "source": [
    "img = read_image(\"test/assets/encode_jpeg/grace_hopper_517x606.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1/10, Step 100/384, Loss: 3.4980\n",
      "Epoch 1/10, Step 200/384, Loss: 3.1426\n",
      "Epoch 1/10, Step 300/384, Loss: 2.8499\n",
      "Training loss: 1274.416, training acc: 22.992\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 2/10, Step 100/384, Loss: 2.8061\n",
      "Epoch 2/10, Step 200/384, Loss: 2.5715\n",
      "Epoch 2/10, Step 300/384, Loss: 2.4516\n",
      "Training loss: 905.068, training acc: 38.521\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 3/10, Step 100/384, Loss: 1.9486\n",
      "Epoch 3/10, Step 200/384, Loss: 2.0481\n",
      "Epoch 3/10, Step 300/384, Loss: 1.9821\n",
      "Training loss: 758.667, training acc: 47.208\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 4/10, Step 100/384, Loss: 1.5668\n",
      "Epoch 4/10, Step 200/384, Loss: 1.3292\n",
      "Epoch 4/10, Step 300/384, Loss: 1.8148\n",
      "Training loss: 655.799, training acc: 53.180\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 5/10, Step 100/384, Loss: 1.5603\n",
      "Epoch 5/10, Step 200/384, Loss: 1.4181\n",
      "Epoch 5/10, Step 300/384, Loss: 1.4595\n",
      "Training loss: 569.838, training acc: 58.945\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 6/10, Step 100/384, Loss: 1.1084\n",
      "Epoch 6/10, Step 200/384, Loss: 1.1083\n",
      "Epoch 6/10, Step 300/384, Loss: 1.5179\n",
      "Training loss: 487.478, training acc: 64.448\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 7/10, Step 100/384, Loss: 1.1333\n",
      "Epoch 7/10, Step 200/384, Loss: 1.0722\n",
      "Epoch 7/10, Step 300/384, Loss: 1.0130\n",
      "Training loss: 413.858, training acc: 69.047\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 8/10, Step 100/384, Loss: 0.9492\n",
      "Epoch 8/10, Step 200/384, Loss: 0.9526\n",
      "Epoch 8/10, Step 300/384, Loss: 0.6476\n",
      "Training loss: 347.031, training acc: 73.633\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 9/10, Step 100/384, Loss: 0.7455\n",
      "Epoch 9/10, Step 200/384, Loss: 0.7558\n",
      "Epoch 9/10, Step 300/384, Loss: 0.8201\n",
      "Training loss: 289.509, training acc: 77.607\n",
      "--------------------------------------------------\n",
      "Training\n",
      "Epoch 10/10, Step 100/384, Loss: 0.4705\n",
      "Epoch 10/10, Step 200/384, Loss: 0.5354\n",
      "Epoch 10/10, Step 300/384, Loss: 0.6996\n",
      "Training loss: 236.153, training acc: 81.266\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = model_resnet18\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "# Start the training.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader_train,0):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass.\n",
    "        outputs = model(image)\n",
    "        # Calculate the loss.\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        # Calculate the accuracy.\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update the weights.\n",
    "        optimizer.step()    # Output training \n",
    "        \n",
    "        if counter % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Step {counter}/{len(dataloader_train)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    train_loss.append(train_running_loss)\n",
    "    # valid_loss.append(valid_epoch_loss)\n",
    "    train_epoch_acc = 100 * train_running_correct/len(train_dataset)\n",
    "    train_acc.append(train_epoch_acc )\n",
    "    # valid_acc.append(valid_epoch_acc)\n",
    "    print(f\"Training loss: {train_running_loss :.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    # print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "    print('-'*50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "val_accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluation on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 100 images from EuroSAT dataset\n",
    "\n",
    "# Load the EuroSAT Categories\n",
    "categories = [name for name in os.listdir('./data/eurosat') if os.path.isdir(os.path.join('./data/eurosat', name))]\n",
    "# Randomly select 5 categories\n",
    "selected_categories = np.random.choice(categories, 5, replace=False)\n",
    "\n",
    "selected_images = []\n",
    "training_images = []\n",
    "\n",
    "# From each directory, randomly select 20 images\n",
    "for category in selected_categories:\n",
    "    images = os.listdir(os.path.join('./data/eurosat', category))\n",
    "    selected = random.sample(images, 20)\n",
    "    selected_images.extend([(category, image) for image in selected])\n",
    "    \n",
    "    # Copy selected images to the selected directory\n",
    "    for image in selected:\n",
    "        shutil.copyfile(os.path.join('./data/eurosat', category, image), os.path.join('./data/eurosat_selected', image))\n",
    "\n",
    "# From these 100 images, randomly select 5 images from each category for the training set\n",
    "for category in selected_categories:\n",
    "    category_images = [image for (cat, image) in selected_images if cat == category]\n",
    "    training = random.sample(category_images, 5)\n",
    "    training_images.extend(training)\n",
    "\n",
    "    # Copy training images to the training directory\n",
    "    for image in training:\n",
    "        copyfile(os.path.join('./data/eurosat_selected', image), os.path.join('./data/eurosat_training', image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
