{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 521153S Deep Learning Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152, ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights\n",
    "import gdown\n",
    "import urllib\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# If you encounter some issues regarding cuda device, e.g., \"RuntimeError: CUDA Out of memory error\",\n",
    "# try to switch the device to cpu by using the following code\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "val_url = 'https://drive.google.com/u/0/uc?id=1hSMUMj5IRpf-nQs1OwgiQLmGZCN0KDWl'\n",
    "train_url = 'https://drive.google.com/u/0/uc?id=107FTosYIeBn5QbynR46YG91nHcJ70whs'\n",
    "test_url = 'https://drive.google.com/u/0/uc?id=1yKyKgxcnGMIAnA_6Vr2ilbpHMc9COg-v'\n",
    "eurosat_url = 'https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1'\n",
    "\n",
    "val_file = './data/val.tar'\n",
    "train_file = './data/train.tar'\n",
    "test_file = './data/test.tar'\n",
    "eurosat_file = './data/eurosatrgb.zip'\n",
    "\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    print('Creating data directory')\n",
    "    os.mkdir('./data')\n",
    "\n",
    "if not os.path.exists('./data/val.tar'):\n",
    "    print('Downloading val.tar')\n",
    "    gdown.download(val_url, val_file)\n",
    "    val_tar = tarfile.open(val_file)\n",
    "    val_tar.extractall('./data/')\n",
    "    val_tar.close()\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print('Downloading train.tar')\n",
    "    gdown.download(train_url, train_file)\n",
    "    train_tar = tarfile.open(train_file)\n",
    "    train_tar.extractall('./data/')\n",
    "    train_tar.close()\n",
    "\n",
    "if not os.path.exists(test_file):\n",
    "    print('Downloading test.tar')\n",
    "    gdown.download(test_url, test_file)\n",
    "    test_tar = tarfile.open(test_file)\n",
    "    test_tar.extractall('./data/')\n",
    "    test_tar.close()\n",
    "\n",
    "if not os.path.exists(eurosat_file):\n",
    "    print('Downloading EuroSAT_RGB.zip')\n",
    "    response = urllib.request.urlretrieve(eurosat_url, eurosat_file)\n",
    "    eurosat_zip = zipfile.ZipFile(eurosat_file)\n",
    "    eurosat_zip.extractall('./data/eurosat')\n",
    "    eurosat_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Size\n",
    "image_size = 32\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 100\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\joona/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\joona\\.conda\\envs\\CUDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "model_resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model_resnet34 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "model_resnet50 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model_resnet101 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "model_resnet152 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/train',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/test',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "validation_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./data/val',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "dataloader_validation = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_resnet18\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device):\n",
    "\n",
    "    model.eval() \n",
    "    print('Evaluate')\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            counter += 1\n",
    "            \n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = valid_running_correct / len(data_loader.dataset)\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader_train, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Start the training.\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA\")\n",
    "\n",
    "    print('Training')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_running_loss = 0.0\n",
    "        train_running_correct = 0\n",
    "        counter = 0\n",
    "\n",
    "        for i, data in enumerate(dataloader_train,0):\n",
    "            counter += 1\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            train_running_correct += (preds == labels).sum().item()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            # Update the weights.\n",
    "            optimizer.step()    # Output training \n",
    "            \n",
    "            if counter % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Step {counter}/{len(dataloader_train)}, Loss: {loss.item():.4f}')\n",
    "        train_epoch_loss = train_running_loss / counter\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        # valid_loss.append(valid_epoch_loss)\n",
    "        train_epoch_acc = train_running_correct/len(train_dataset)\n",
    "        train_acc.append(train_epoch_acc )\n",
    "        # valid_acc.append(valid_epoch_acc)\n",
    "        print(f\"Training loss: {train_epoch_loss :.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "        # print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "        print('-'*50)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training\n",
      "Epoch 1/10, Step 100/384, Loss: 0.3904\n",
      "Epoch 1/10, Step 200/384, Loss: 0.4916\n",
      "Epoch 1/10, Step 300/384, Loss: 0.4226\n",
      "Training loss: 0.525, training acc: 0.842\n",
      "--------------------------------------------------\n",
      "Epoch 2/10, Step 100/384, Loss: 0.3059\n",
      "Epoch 2/10, Step 200/384, Loss: 0.4189\n",
      "Epoch 2/10, Step 300/384, Loss: 0.4981\n",
      "Training loss: 0.428, training acc: 0.869\n",
      "--------------------------------------------------\n",
      "Epoch 3/10, Step 100/384, Loss: 0.2737\n",
      "Epoch 3/10, Step 200/384, Loss: 0.3679\n",
      "Epoch 3/10, Step 300/384, Loss: 0.3490\n",
      "Training loss: 0.343, training acc: 0.897\n",
      "--------------------------------------------------\n",
      "Epoch 4/10, Step 100/384, Loss: 0.2759\n",
      "Epoch 4/10, Step 200/384, Loss: 0.3452\n",
      "Epoch 4/10, Step 300/384, Loss: 0.4014\n",
      "Training loss: 0.278, training acc: 0.916\n",
      "--------------------------------------------------\n",
      "Epoch 5/10, Step 100/384, Loss: 0.2886\n",
      "Epoch 5/10, Step 200/384, Loss: 0.2013\n",
      "Epoch 5/10, Step 300/384, Loss: 0.0879\n",
      "Training loss: 0.222, training acc: 0.933\n",
      "--------------------------------------------------\n",
      "Epoch 6/10, Step 100/384, Loss: 0.1604\n",
      "Epoch 6/10, Step 200/384, Loss: 0.1813\n",
      "Epoch 6/10, Step 300/384, Loss: 0.2060\n",
      "Training loss: 0.194, training acc: 0.941\n",
      "--------------------------------------------------\n",
      "Epoch 7/10, Step 100/384, Loss: 0.1104\n",
      "Epoch 7/10, Step 200/384, Loss: 0.2053\n",
      "Epoch 7/10, Step 300/384, Loss: 0.1736\n",
      "Training loss: 0.158, training acc: 0.952\n",
      "--------------------------------------------------\n",
      "Epoch 8/10, Step 100/384, Loss: 0.0619\n",
      "Epoch 8/10, Step 200/384, Loss: 0.1512\n",
      "Epoch 8/10, Step 300/384, Loss: 0.1573\n",
      "Training loss: 0.145, training acc: 0.958\n",
      "--------------------------------------------------\n",
      "Epoch 9/10, Step 100/384, Loss: 0.0675\n",
      "Epoch 9/10, Step 200/384, Loss: 0.1300\n",
      "Epoch 9/10, Step 300/384, Loss: 0.2017\n",
      "Training loss: 0.120, training acc: 0.964\n",
      "--------------------------------------------------\n",
      "Epoch 10/10, Step 100/384, Loss: 0.0526\n",
      "Epoch 10/10, Step 200/384, Loss: 0.0420\n",
      "Epoch 10/10, Step 300/384, Loss: 0.0480\n",
      "Training loss: 0.098, training acc: 0.971\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(model, dataloader_train, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate\n",
      "Validation loss: 12.844, validation acc: 0.011\n",
      "Evaluate\n",
      "Test loss: 13.281, test acc: 0.008\n"
     ]
    }
   ],
   "source": [
    "validate_loss, validate_acc = evaluate(trained_model, dataloader_validation, criterion, device)\n",
    "print(f\"Validation loss: {validate_loss:.3f}, validation acc: {validate_acc:.3f}\")\n",
    "test_loss, test_acc = evaluate(trained_model, dataloader_test, criterion, device)\n",
    "print(f\"Test loss: {test_loss:.3f}, test acc: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 5 categories from ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake'] categories\n",
      "Selected categories: ['AnnualCrop' 'Forest' 'Residential' 'Pasture' 'SeaLake']\n",
      "Selected ['AnnualCrop_883.jpg', 'AnnualCrop_702.jpg', 'AnnualCrop_267.jpg', 'AnnualCrop_2033.jpg', 'AnnualCrop_1677.jpg', 'AnnualCrop_239.jpg', 'AnnualCrop_903.jpg', 'AnnualCrop_1860.jpg', 'AnnualCrop_505.jpg', 'AnnualCrop_107.jpg', 'AnnualCrop_1712.jpg', 'AnnualCrop_2873.jpg', 'AnnualCrop_2692.jpg', 'AnnualCrop_487.jpg', 'AnnualCrop_635.jpg', 'AnnualCrop_233.jpg', 'AnnualCrop_898.jpg', 'AnnualCrop_559.jpg', 'AnnualCrop_459.jpg', 'AnnualCrop_2848.jpg'] from AnnualCrop\n",
      "Selected ['Forest_1081.jpg', 'Forest_2765.jpg', 'Forest_2330.jpg', 'Forest_2934.jpg', 'Forest_606.jpg', 'Forest_2711.jpg', 'Forest_2992.jpg', 'Forest_2585.jpg', 'Forest_116.jpg', 'Forest_1246.jpg', 'Forest_95.jpg', 'Forest_2285.jpg', 'Forest_1543.jpg', 'Forest_2418.jpg', 'Forest_1162.jpg', 'Forest_512.jpg', 'Forest_1676.jpg', 'Forest_160.jpg', 'Forest_1509.jpg', 'Forest_188.jpg'] from Forest\n",
      "Selected ['Residential_1501.jpg', 'Residential_98.jpg', 'Residential_117.jpg', 'Residential_2724.jpg', 'Residential_153.jpg', 'Residential_350.jpg', 'Residential_2403.jpg', 'Residential_759.jpg', 'Residential_582.jpg', 'Residential_895.jpg', 'Residential_1257.jpg', 'Residential_2666.jpg', 'Residential_2347.jpg', 'Residential_2855.jpg', 'Residential_915.jpg', 'Residential_2771.jpg', 'Residential_1911.jpg', 'Residential_1470.jpg', 'Residential_1979.jpg', 'Residential_1308.jpg'] from Residential\n",
      "Selected ['Pasture_907.jpg', 'Pasture_895.jpg', 'Pasture_330.jpg', 'Pasture_1244.jpg', 'Pasture_1690.jpg', 'Pasture_1590.jpg', 'Pasture_757.jpg', 'Pasture_1456.jpg', 'Pasture_1239.jpg', 'Pasture_1062.jpg', 'Pasture_445.jpg', 'Pasture_973.jpg', 'Pasture_1204.jpg', 'Pasture_31.jpg', 'Pasture_1970.jpg', 'Pasture_1313.jpg', 'Pasture_579.jpg', 'Pasture_1765.jpg', 'Pasture_841.jpg', 'Pasture_449.jpg'] from Pasture\n",
      "Selected ['SeaLake_1640.jpg', 'SeaLake_948.jpg', 'SeaLake_2941.jpg', 'SeaLake_2353.jpg', 'SeaLake_1257.jpg', 'SeaLake_769.jpg', 'SeaLake_2558.jpg', 'SeaLake_428.jpg', 'SeaLake_2501.jpg', 'SeaLake_1496.jpg', 'SeaLake_998.jpg', 'SeaLake_2130.jpg', 'SeaLake_850.jpg', 'SeaLake_2907.jpg', 'SeaLake_1018.jpg', 'SeaLake_2636.jpg', 'SeaLake_2903.jpg', 'SeaLake_2875.jpg', 'SeaLake_447.jpg', 'SeaLake_1375.jpg'] from SeaLake\n",
      "Selected ['AnnualCrop_2873.jpg', 'AnnualCrop_487.jpg', 'AnnualCrop_2033.jpg', 'AnnualCrop_233.jpg', 'AnnualCrop_1677.jpg'] for training\n",
      "Selected ['Forest_2711.jpg', 'Forest_2330.jpg', 'Forest_1246.jpg', 'Forest_2285.jpg', 'Forest_1081.jpg'] for training\n",
      "Selected ['Residential_1308.jpg', 'Residential_1979.jpg', 'Residential_1501.jpg', 'Residential_2403.jpg', 'Residential_915.jpg'] for training\n",
      "Selected ['Pasture_579.jpg', 'Pasture_445.jpg', 'Pasture_895.jpg', 'Pasture_31.jpg', 'Pasture_330.jpg'] for training\n",
      "Selected ['SeaLake_769.jpg', 'SeaLake_428.jpg', 'SeaLake_2130.jpg', 'SeaLake_948.jpg', 'SeaLake_850.jpg'] for training\n"
     ]
    }
   ],
   "source": [
    "# Choose 100 images from EuroSAT dataset\n",
    "\n",
    "if not os.path.exists('./data/eurosat_selected'):\n",
    "    print('Creating data directory')\n",
    "    os.mkdir('./data/eurosat_selected')\n",
    "\n",
    "if not os.path.exists('./data/eurosat_training'):\n",
    "    print('Creating training directory')\n",
    "    os.mkdir('./data/eurosat_training')\n",
    "\n",
    "# Load the EuroSAT Categories\n",
    "eurosat_categories = [name for name in os.listdir('./data/eurosat/EuroSAT_RGB/') if os.path.isdir(os.path.join('./data/eurosat/EuroSAT_RGB/', name))]\n",
    "# Randomly select 5 categories\n",
    "print(f\"Selecting 5 categories from {eurosat_categories} categories\")\n",
    "selected_categories = np.random.choice(eurosat_categories, 5, replace=False)\n",
    "print(f\"Selected categories: {selected_categories}\")\n",
    "\n",
    "selected_images = []\n",
    "training_images = []\n",
    "\n",
    "# From each directory, randomly select 20 images\n",
    "for category in selected_categories:\n",
    "    images = os.listdir(os.path.join('./data/eurosat/EuroSAT_RGB/', category))\n",
    "    selected = random.sample(images, 20)\n",
    "    selected_images.extend([(category, image) for image in selected])\n",
    "    print(f\"Selected {selected} from {category}\")\n",
    "    # Copy selected images to the selected directory\n",
    "    for image in selected:\n",
    "        shutil.copyfile(os.path.join('./data/eurosat/EuroSAT_RGB', category, image), os.path.join('./data/eurosat_selected', image))\n",
    "\n",
    "# From these 100 images, randomly select 5 images from each category for the training set\n",
    "for category in selected_categories:\n",
    "    category_images = [image for (cat, image) in selected_images if cat == category]\n",
    "    training = random.sample(category_images, 5)\n",
    "    training_images.extend(training)\n",
    "    print(f\"Selected {training} for training\")\n",
    "\n",
    "    # Copy training images to the training directory\n",
    "    for image in training:\n",
    "        shutil.copyfile(os.path.join('./data/eurosat_selected', image), os.path.join('./data/eurosat_training', image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
