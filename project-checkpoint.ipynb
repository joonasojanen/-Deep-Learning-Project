{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 521153S Deep Learning Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Koodi\\Deep-Learning-Project\\project-checkpoint.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Koodi/Deep-Learning-Project/project-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Koodi/Deep-Learning-Project/project-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgdown\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Koodi/Deep-Learning-Project/project-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwget\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Koodi/Deep-Learning-Project/project-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Koodi/Deep-Learning-Project/project-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtarfile\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import gdown\n",
    "import wget\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download val, test and traininig data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading val.tar\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/val.tar\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloading val.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mgdown\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(val_url, filename\u001b[38;5;241m=\u001b[39mval_file, quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m     val_tar \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/val.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     val_tar\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/validation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gdown' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# download the dataset\n",
    "val_url = 'https://drive.google.com/file/d/1hSMUMj5IRpf-nQs1OwgiQLmGZCN0KDWl'\n",
    "train_url = 'https://drive.google.com/file/d/107FTosYIeBn5QbynR46YG91nHcJ70whs'\n",
    "test_url = 'https://drive.google.com/file/d/1yKyKgxcnGMIAnA_6Vr2ilbpHMc9COg-v'\n",
    "eurosat_url = 'https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1'\n",
    "\n",
    "val_file = './data/val.tar'\n",
    "train_file = './data/train.tar'\n",
    "test_file = './data/test.tar'\n",
    "eurosat_file = './data/eurosatrgb.zip'\n",
    "\n",
    "if not os.path.exists('./data/val.tar'):\n",
    "    print('Downloading val.tar')\n",
    "    gdown.download(val_url, filename=val_file, quiet=False)\n",
    "    val_tar = tarfile.open('./data/val.tar')\n",
    "    val_tar.extractall('./data/validation')\n",
    "    val_tar.close()\n",
    "\n",
    "if not os.path.exists('./data/train.tar'):\n",
    "    print('Downloading train.tar')\n",
    "    gdown.download(train_url, filename=train_file, quiet=False)\n",
    "    train_tar = tarfile.open('./data/train.tar')\n",
    "    train_tar.extractall('./data/train')\n",
    "    train_tar.close()\n",
    "\n",
    "if not os.path.exists('./data/test.tar'):\n",
    "    print('Downloading test.tar')\n",
    "    gdown.download(test_url, filename=test_file, quiet=False)\n",
    "    test_tar = tarfile.open('./data/test.tar')\n",
    "    test_tar.extractall('./data/test')\n",
    "    test_tar.close()\n",
    "\n",
    "if not os.path.exists('./data/eurosatrgb.zip'):\n",
    "    print('Downloading EuroSAT_RGB.zip')\n",
    "    response = wget.download(eurosat_url, eurosat_file)\n",
    "    eurosat_zip = zipfile.ZipFile(eurosat_file)\n",
    "    eurosat_zip.extractall('./data/eurosat')\n",
    "    eurosat_zip.close()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will switch to cuda device automatically to accelerate your code if gpu is available in your environment.\n",
    "\n",
    "To enable GPU in Google Colab, you can navigate to `Edit → Notebook Settings → Hardware Accelerator` , and then select `T4 GPU` or `TPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# If you encounter some issues regarding cuda device, e.g., \"RuntimeError: CUDA Out of memory error\",\n",
    "# try to switch the device to cpu by using the following code\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader_train, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Start the training.\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA\")\n",
    "\n",
    "    print('Training')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_running_loss = 0.0\n",
    "        train_running_correct = 0\n",
    "        counter = 0\n",
    "\n",
    "        for i, data in enumerate(dataloader_train,0):\n",
    "            counter += 1\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            train_running_correct += (preds == labels).sum().item()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            # Update the weights.\n",
    "            optimizer.step()    # Output training \n",
    "            \n",
    "            if counter % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Step {counter}/{len(dataloader_train)}, Loss: {loss.item():.4f}')\n",
    "        train_epoch_loss = train_running_loss / counter\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        # valid_loss.append(valid_epoch_loss)\n",
    "        train_epoch_acc = 100 * train_running_correct/len(train_dataset)\n",
    "        train_acc.append(train_epoch_acc )\n",
    "        # valid_acc.append(valid_epoch_acc)\n",
    "        print(f\"Training loss: {train_epoch_loss :.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "        # print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "        print('-'*50)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 100 images from EuroSAT dataset\n",
    "\n",
    "if not os.path.exists('./data/eurosat_selected'):\n",
    "    print('Creating data directory')\n",
    "    os.mkdir('./data/eurosat_selected')\n",
    "\n",
    "if not os.path.exists('./data/eurosat_training'):\n",
    "    print('Creating training directory')\n",
    "    os.mkdir('./data/eurosat_training')\n",
    "\n",
    "# Load the EuroSAT Categories\n",
    "eurosat_categories = [name for name in os.listdir('./data/eurosat/EuroSAT_RGB/') if os.path.isdir(os.path.join('./data/eurosat/EuroSAT_RGB/', name))]\n",
    "# Randomly select 5 categories\n",
    "print(f\"Selecting 5 categories from {eurosat_categories} categories\")\n",
    "selected_categories = np.random.choice(eurosat_categories, 5, replace=False)\n",
    "print(f\"Selected categories: {selected_categories}\")\n",
    "\n",
    "selected_images = []\n",
    "training_images = []\n",
    "\n",
    "# From each directory, randomly select 20 images\n",
    "for category in selected_categories:\n",
    "    images = os.listdir(os.path.join('./data/eurosat/EuroSAT_RGB/', category))\n",
    "    selected = random.sample(images, 20)\n",
    "    selected_images.extend([(category, image) for image in selected])\n",
    "    print(f\"Selected {selected} from {category}\")\n",
    "    # Copy selected images to the selected directory\n",
    "    for image in selected:\n",
    "        shutil.copyfile(os.path.join('./data/eurosat/EuroSAT_RGB', category, image), os.path.join('./data/eurosat_selected', image))\n",
    "\n",
    "# From these 100 images, randomly select 5 images from each category for the training set\n",
    "for category in selected_categories:\n",
    "    category_images = [image for (cat, image) in selected_images if cat == category]\n",
    "    training = random.sample(category_images, 5)\n",
    "    training_images.extend(training)\n",
    "    print(f\"Selected {training} for training\")\n",
    "\n",
    "    # Copy training images to the training directory\n",
    "    for image in training:\n",
    "        shutil.copyfile(os.path.join('./data/eurosat_selected', image), os.path.join('./data/eurosat_training', image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
